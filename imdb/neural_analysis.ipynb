{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import pandas as pd\n",
    "import mlflow\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "import os\n",
    "\n",
    "from pathlib import Path\n",
    "import logging\n",
    "from enum import StrEnum\n",
    "\n",
    "from src.data_handling import process_training_data\n",
    "from src.mlflow_setup import mlflow_setup\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "class ModelType(StrEnum):\n",
    "    SPARSE = \"sparse\"\n",
    "    DENSE = \"dense\"\n",
    "\n",
    "model_type = ModelType.SPARSE\n",
    "\n",
    "match model_type:\n",
    "    case ModelType.SPARSE:\n",
    "        from src.neural_analysis.sparse import ImdbDataSet, Model\n",
    "    case ModelType.DENSE:\n",
    "        from src.neural_analysis.dense import ImdbDataSet, Model\n",
    "    case _:\n",
    "        raise Exception\n",
    "\n",
    "mlflow_setup()\n",
    "mlflow.set_experiment(\"Neural analysis\")\n",
    "mlflow.end_run()\n",
    "mlflow_run = mlflow.start_run(description=model_type)\n",
    "tb_writer = SummaryWriter(comment=model_type)\n",
    "\n",
    "data_path = Path() / \"data\"\n",
    "source_data_path = data_path / \"imdb.csv\"\n",
    "imdb_data = pd.read_csv(source_data_path)\n",
    "\n",
    "\n",
    "full_context_counts_path = data_path / \"full_context_counts.csv\"\n",
    "if not (os.path.exists(full_context_counts_path)):\n",
    "    process_training_data(\n",
    "        imdb_data.review,\n",
    "        imdb_data.sentiment,\n",
    "        full_context_counts_path\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_more_than = 19\n",
    "mlflow.log_param(\"words_more_than\", words_more_than)\n",
    "context_counts = pd.read_csv(full_context_counts_path).query(f\"total > {words_more_than}\")\n",
    "# neg_pos_diff = (context_counts[\"negative\"]-context_counts[\"positive\"])/context_counts[\"total\"]\n",
    "# neg_pos_diff_more_than = 0.2\n",
    "# mlflow.log_param(\"neg_pos_diff_more_than\", neg_pos_diff_more_than)\n",
    "# context_counts = context_counts[neg_pos_diff.abs() > neg_pos_diff_more_than]\n",
    "print(context_counts.word.is_unique)\n",
    "mlflow.log_input(\n",
    "    mlflow.data.from_pandas(\n",
    "        context_counts,\n",
    "        source=str(full_context_counts_path),\n",
    "        name=\"context counts\"\n",
    "    )\n",
    ")\n",
    "context_counts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.data_handling import process_test_data\n",
    "\n",
    "test_data_path = data_path / \"stemmed_data.csv\"\n",
    "\n",
    "if not (os.path.exists(test_data_path)):\n",
    "    process_test_data(\n",
    "        imdb_data.review,\n",
    "        imdb_data.sentiment,\n",
    "        test_data_path\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.data_handling import SparseNumericTestDataIO, StandardLengthTestData\n",
    "\n",
    "import torch\n",
    "\n",
    "# write new numeric test data based on context_counts\n",
    "# ---------------------------------------------------\n",
    "\n",
    "test_data = pd.read_csv(test_data_path)\n",
    "mlflow.log_input(\n",
    "    mlflow.data.from_pandas(\n",
    "        test_data,\n",
    "        source=str(test_data_path),\n",
    "        name=\"test data\"\n",
    "    )\n",
    ")\n",
    "\n",
    "used_words = context_counts.word\n",
    "numeric_test_data_path = data_path / \"sparse_numeric_stemmed_data.dat\"\n",
    "compute = True\n",
    "if compute:\n",
    "    SparseNumericTestDataIO(\n",
    "        test_data.context,\n",
    "        test_data.words,\n",
    "        used_words\n",
    "    ).write(numeric_test_data_path)\n",
    "\n",
    "numeric_test_data = SparseNumericTestDataIO.read(numeric_test_data_path)\n",
    "if model_type == ModelType.SPARSE:\n",
    "    sentence_length = 0.99\n",
    "    numeric_test_data = StandardLengthTestData.from_sparse(numeric_test_data, sentence_length=sentence_length)\n",
    "# =========================================================\n",
    "\n",
    "# the sparse numeric adds <unk> token (and possibly others), so redo used_words\n",
    "used_words = pd.Series(numeric_test_data.train_words_dict.values())\n",
    "used_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "dtype = torch.float32\n",
    "\n",
    "\n",
    "train_size, eval_size, test_size = 40000, 5000, 5000\n",
    "# It's not really test data, because it's now used as\n",
    "# the training for the model, but the previous data is\n",
    "# training data also, as it's used as the basis for\n",
    "# processing this here \"test\" data.\n",
    "mlflow.log_param(\"train set size\", train_size)\n",
    "train_dataset = ImdbDataSet(\n",
    "    numeric_test_data,\n",
    "    end_row=train_size,\n",
    "    device=device\n",
    ")\n",
    "mlflow.log_param(\"eval set size\", eval_size)\n",
    "eval_dataset = ImdbDataSet(\n",
    "    numeric_test_data,\n",
    "    start_row=train_size,\n",
    "    end_row=train_size+eval_size,\n",
    "    device=device\n",
    ")\n",
    "test_dataset = ImdbDataSet(\n",
    "    numeric_test_data,\n",
    "    start_row=train_size+eval_size,\n",
    "    end_row=train_size+eval_size+test_size,\n",
    "    device=device\n",
    ")\n",
    "mlflow.log_param(\"test set size\", len(test_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(train_dataset), len(test_dataset))\n",
    "words, context = train_dataset[:]\n",
    "words = words.cpu()\n",
    "print(\"Data sparsity:\", words.to(dtype=bool).to(dtype=float).mean())\n",
    "del words\n",
    "del context\n",
    "train_dataset[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how many training words are being used\n",
    "num_used_words = len(used_words)\n",
    "\n",
    "embedding_dim = 3\n",
    "mlflow.log_param(\"embedding dimension\", embedding_dim)\n",
    "\n",
    "\n",
    "args = [num_used_words, embedding_dim]\n",
    "if model_type is ModelType.SPARSE:\n",
    "    args.append(numeric_test_data.sentence_length)\n",
    "        \n",
    "model = Model(*args, device=device)\n",
    "artifacts_path = Path() / \"artifacts\"\n",
    "artifacts_path.mkdir(exist_ok=True)\n",
    "model_desc_path = artifacts_path / \"model_description.txt\"\n",
    "with open(model_desc_path, \"w\") as f:\n",
    "    print(model, file=f)\n",
    "\n",
    "mlflow.log_artifact(model_desc_path)\n",
    "\n",
    "print(model.embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "batch_size = 300\n",
    "shuffle = True\n",
    "mlflow.log_param(\"batch size\", batch_size)\n",
    "mlflow.log_param(\"shuffle during training\", shuffle)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=shuffle)\n",
    "_ = next(iter(train_dataloader))\n",
    "print(_)\n",
    "print(_[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words, context = train_dataset[:5]\n",
    "pred = model(words.to(dtype = torch.float32))\n",
    "\n",
    "tb_writer.add_graph(model, words.to(dtype = torch.float32))\n",
    "\n",
    "print(pred)\n",
    "print(context)\n",
    "predicted_class = pred.argmax(dim=1)\n",
    "print(f\"{predicted_class=}\")\n",
    "nlll = torch.nn.NLLLoss()\n",
    "print(nlll(pred, context))\n",
    "-pred[range(len(context)), context].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The idea here is to train the vectors for the words as the weight matrix\n",
    "of the first layer. The matrix would get updated according to the loss,\n",
    "so that a vector that matches a negative word should result in a more\n",
    "negative guess, and similarly for positive words. Further, words that\n",
    "appear together get updated similarly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "\n",
    "model = Model(*args, device=device)\n",
    "loss_fn = torch.nn.NLLLoss()\n",
    "loss_path = artifacts_path / \"loss.txt\"\n",
    "with open(loss_path, \"w\") as f:\n",
    "    print(loss_fn, file=f)\n",
    "mlflow.log_artifact(loss_path)\n",
    "optim = torch.optim.Adam(model.parameters(), lr = 1e-3)\n",
    "optim_path = artifacts_path / \"optim.txt\"\n",
    "with open(optim_path, \"w\") as f:\n",
    "    print(optim, file=f)\n",
    "\n",
    "def test(model, test_data):\n",
    "\n",
    "    words, context = test_data[:]\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "\n",
    "        pred = model(words).argmax(dim=1)\n",
    "        \n",
    "        acc = 1-torch.abs(context-pred).mean(dtype=float)\n",
    "\n",
    "\n",
    "    model.train()\n",
    "    return acc.item()\n",
    "\n",
    "def scaled_normalisation(embedding:torch.Tensor, has_padding = True):\n",
    "    if has_padding:\n",
    "        new_embed = embedding[1:, :]\n",
    "    else:\n",
    "        new_embed = embedding\n",
    "    new_embed -= new_embed.min(dim=0).values-1e-6\n",
    "    new_embed /= torch.max(new_embed, torch.ones_like(new_embed)*1e-6).max(dim=0).values\n",
    "    # new_embed = new_embed*2.0 - 1.0\n",
    "    if has_padding:\n",
    "        embedding[1:, :] = new_embed\n",
    "    else:\n",
    "        embedding = new_embed\n",
    "    return embedding\n",
    "\n",
    "def unit_normalisation_with_constant(embedding:torch.Tensor):\n",
    "    new_embed = scaled_normalisation(embedding)\n",
    "    new_embed[0,:] = 1.0\n",
    "    return new_embed\n",
    "\n",
    "def normalisation(embedding:torch.Tensor, has_padding = True):\n",
    "    embedding = scaled_normalisation(embedding, has_padding)\n",
    "    # embedding[0,0] = 0.1\n",
    "    # embedding[1:,0] = -0.1\n",
    "    return embedding\n",
    "\n",
    "total_epochs = 20\n",
    "mlflow.log_param(\"epochs\", total_epochs)\n",
    "\n",
    "\n",
    "losses = []\n",
    "train_accs = []\n",
    "eval_accs = []\n",
    "max_eval = 0.0\n",
    "for epoch in range(total_epochs):\n",
    "\n",
    "    epoch_loss = []\n",
    "    for i, (words, context) in enumerate(train_dataloader):\n",
    "\n",
    "        pred = model(words)\n",
    "        loss = loss_fn(pred, context)/batch_size\n",
    "\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "\n",
    "        # normalise\n",
    "        # ------------\n",
    "        match model_type:\n",
    "            case ModelType.DENSE:\n",
    "                model.embeddings = normalisation(model.embeddings, False)\n",
    "            case _:\n",
    "                pass\n",
    "        # ==================\n",
    "\n",
    "        optim.zero_grad()\n",
    "\n",
    "        epoch_loss.append(loss.item())\n",
    "\n",
    "\n",
    "    mean_epoch_loss = torch.tensor(epoch_loss).mean().item()\n",
    "\n",
    "    tb_writer.add_scalar(\"mean_epoch_loss/train\", mean_epoch_loss, global_step=epoch)\n",
    "    mlflow.log_metric(\"mean epoch loss\", mean_epoch_loss, step=epoch)\n",
    "    losses.extend(epoch_loss)\n",
    "\n",
    "    train_acc = test(model, train_dataset)\n",
    "    mlflow.log_metric(\"train accuracy\", train_acc, step=epoch)\n",
    "    print(f\"{train_acc=}\")\n",
    "    train_accs.append(train_acc)\n",
    "\n",
    "    eval_acc = test(model, eval_dataset)\n",
    "    mlflow.log_metric(\"eval accuracy\", eval_acc, step=epoch)\n",
    "    if eval_acc > max_eval:\n",
    "        best_state_dict = deepcopy(model.state_dict())\n",
    "        max_eval = eval_acc\n",
    "        best_epoch = epoch\n",
    "\n",
    "    tb_writer.add_scalars(\"accuracy\", dict(\n",
    "        evaluation=eval_acc,\n",
    "        train=train_acc\n",
    "    ), global_step=epoch)\n",
    "\n",
    "    print(f\"{eval_acc=}\")\n",
    "    eval_accs.append(eval_acc)\n",
    "\n",
    "losses = torch.tensor(losses)\n",
    "\n",
    "model.load_state_dict(best_state_dict)\n",
    "tb_writer.add_embedding(model.embeddings.cpu(), used_words, global_step=best_epoch)\n",
    "sig = mlflow.models.infer_signature(\n",
    "    words.cpu().numpy(),\n",
    "    model(words).detach().cpu().numpy()\n",
    ")\n",
    "mlflow.pytorch.log_model(model, \"model\", signature=sig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.style as mplstyle\n",
    "\n",
    "import itertools\n",
    "\n",
    "mplstyle.use([\"fast\"])\n",
    "\n",
    "print(losses.size())\n",
    "\n",
    "window = 50\n",
    "rolling_average_loss = list(\n",
    "    map(lambda val: sum(val)/window, itertools.batched(losses, window))\n",
    ")\n",
    "print(f\"Mean: {losses.mean().item()}\")\n",
    "print(f\"Var: {losses.var().item()}\")\n",
    "print(f\"{eval_acc=}\")\n",
    "plt.close(\"all\")\n",
    "plt.plot(\n",
    "    rolling_average_loss,\n",
    ")\n",
    "_ = plt.title(f\"Rolling average loss, window: {window}\\n epochs: {total_epochs} data per epoch: {len(train_dataset)} batch size: {batch_size}\")\n",
    "# plt.plot(sorted(preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(enumerate(used_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get vector representations of the words from the model\n",
    "word_vecs = model.embeddings.cpu().clone().detach()\n",
    "chosen_idx = int(used_words[used_words == \"bad\"].index[0])\n",
    "chosen_idx = int(used_words[used_words == \"excel\"].index[0])\n",
    "print(\"Chosen idx: \", chosen_idx)\n",
    "# vector representation of the word\n",
    "chosen_word = word_vecs[chosen_idx]\n",
    "# actual word itself\n",
    "print(\"Chosen word:\", used_words[chosen_idx])\n",
    "\n",
    "# determine how similar each of the other words is to this one\n",
    "cos = torch.nn.PairwiseDistance()\n",
    "similarities = torch.tensor([cos(chosen_word, word_vecs[i]) for i in range(len(word_vecs))])\n",
    "most_similar = torch.argsort(similarities, descending=False)\n",
    "plt.plot(similarities.flatten().sort()[0])\n",
    "similars_list = used_words[most_similar.numpy()].to_list()\n",
    "print(*similars_list, sep=\"\\n\", file=open(\"temp.dat\", \"w\"))\n",
    "pd.DataFrame(dict(most_similar=similars_list[:20], least_similar=reversed(similars_list[-20:])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vecs[int(used_words[used_words == \"<unk>\"].index[0])]\n",
    "word_vecs.min(dim=0), word_vecs.max(dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib widget\n",
    "\n",
    "plt.close(\"all\")\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(projection=\"3d\")\n",
    "ax.scatter(*word_vecs.T[[0,1],:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "\n",
    "words, context = test_dataset[:]\n",
    "model.eval()\n",
    "pred: torch.Tensor = model(words).detach()\n",
    "print(pred)\n",
    "print(torch.exp(pred))\n",
    "print(context)\n",
    "pred_class = pred.argmax(dim=1)\n",
    "acc = 1-torch.abs(context - pred_class).mean(dtype=float)\n",
    "mlflow.log_metric(\"test accuracy\", acc)\n",
    "print(f\"{acc=}\")\n",
    "pred = torch.exp(pred)\n",
    "# how far the predictions were from the true value\n",
    "surprise = 1-torch.gather(pred, 1, context.reshape((-1,1)))\n",
    "surprise = surprise.flatten().to(device=\"cpu\")\n",
    "print(surprise)\n",
    "\n",
    "surprise_mean = surprise.mean(dtype=float)\n",
    "print(f\"Mean of surprises: {surprise_mean}\")\n",
    "plt.close(\"all\")\n",
    "plt.hist(surprise)\n",
    "plt.title(\"Distribution of surprises\")\n",
    "mlflow.log_figure(plt.gcf(), \"surprise.png\")\n",
    "plt.figure()\n",
    "sorted_surprise = surprise.sort()\n",
    "plt.plot(sorted_surprise.values)\n",
    "plt.title(\"Sorted surprises\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_surprise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "con = test_dataset.context.clone().cpu().numpy()\n",
    "\n",
    "ft_np = [None]*len(con)\n",
    "for i, (context_word, context_num, pred_context_num) in enumerate(zip(test_dataset.numeric_context_to_word(), con, pred_class)):\n",
    "\n",
    "    pred_type = str(bool(context_num == pred_context_num)).lower() + f\"_{context_word}\"\n",
    "    ft_np[i] = pred_type\n",
    "\n",
    "ft_counts = Counter()\n",
    "ft_counts.update(ft_np)\n",
    "ft_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow.end_run()\n",
    "tb_writer.close()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
