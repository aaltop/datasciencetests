{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.data_handling import process_training_data\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "imdb_data = pd.read_csv(\"./data/imdb.csv\")\n",
    "imdb_data\n",
    "\n",
    "data_path = Path() / \"data\"\n",
    "\n",
    "full_context_counts_path = data_path / \"full_context_counts.csv\"\n",
    "if not (os.path.exists(full_context_counts_path)):\n",
    "    process_training_data(\n",
    "        imdb_data.review,\n",
    "        imdb_data.sentiment,\n",
    "        full_context_counts_path\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "context_counts = pd.read_csv(full_context_counts_path).query(\"total > 99\")\n",
    "print(context_counts.word.is_unique)\n",
    "context_counts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.data_handling import process_test_data\n",
    "\n",
    "test_data_path = data_path / \"stemmed_data.csv\"\n",
    "\n",
    "if not (os.path.exists(test_data_path)):\n",
    "    process_test_data(\n",
    "        imdb_data.review,\n",
    "        imdb_data.sentiment,\n",
    "        test_data_path\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.data_handling import test_data_to_numeric\n",
    "\n",
    "import torch\n",
    "\n",
    "# write new numeric test data based on context_counts\n",
    "# ---------------------------------------------------\n",
    "\n",
    "test_data = pd.read_csv(test_data_path)\n",
    "\n",
    "used_words = context_counts.word[:100]\n",
    "numeric_test_data_path = \"./data/numeric_stemmed_data.csv\"\n",
    "if not (os.path.exists(numeric_test_data_path)):\n",
    "    test_data_to_numeric(\n",
    "        test_data.context,\n",
    "        test_data.words,\n",
    "        used_words,\n",
    "        numeric_test_data_path\n",
    "    )\n",
    "# =========================================================\n",
    "\n",
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "dtype = torch.float32\n",
    "class ImdbDataSet(torch.utils.data.TensorDataset):\n",
    "\n",
    "    def __init__(self, data_path, start_row=0, end_row = None, device = None):\n",
    "        '''\n",
    "        Arguments:\n",
    "            dtype:\n",
    "                should be of integer dtype suitable for torch tensors\n",
    "        '''\n",
    "\n",
    "\n",
    "        data = pd.read_parquet(data_path)[start_row:]\n",
    "        if not (end_row is None):\n",
    "            data = data[:end_row]\n",
    "\n",
    "        # create context mappings\n",
    "        context_categorical = pd.Categorical(data.context)\n",
    "        self.context = context_categorical.codes\n",
    "        self.context_mapping = dict(zip(context_categorical.categories, self.context))\n",
    "\n",
    "        self.device = device or \"cpu\"\n",
    "        # needs to be int64 for nll loss, seemingly\n",
    "        # could also just directly calculate though, which would\n",
    "        # allow using at least int32\n",
    "        self.context = torch.tensor(self.context, dtype=torch.int64, device=device)\n",
    "        \n",
    "        self.words = torch.tensor(data.iloc[:,1:].to_numpy(), dtype=torch.float32, device=self.device)\n",
    "\n",
    "    def __len__(self):\n",
    "\n",
    "        return len(self.context)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        return self.words[idx], self.context[idx]\n",
    "\n",
    "    def numeric_context_to_word(self):\n",
    "\n",
    "        return list(map(lambda val: self.context_mapping[str(int(val))], self.context))\n",
    "\n",
    "\n",
    "# It's not really test data, because it's now used as\n",
    "# the training for the model, but the previous data is\n",
    "# training data also, as it's used as the basis for\n",
    "# processing this here \"test\" data.\n",
    "train_size = 40000\n",
    "train_dataset = ImdbDataSet(\n",
    "    numeric_test_data_path,\n",
    "    end_row=train_size,\n",
    "    device=device\n",
    ")\n",
    "eval_size = 5000\n",
    "eval_dataset = ImdbDataSet(\n",
    "    numeric_test_data_path,\n",
    "    start_row=train_size,\n",
    "    end_row=train_size+eval_size,\n",
    "    device=device\n",
    ")\n",
    "test_dataset = ImdbDataSet(\n",
    "    numeric_test_data_path,\n",
    "    start_row=train_size+eval_size,\n",
    "    device=device\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(train_dataset), len(test_dataset))\n",
    "words, context = train_dataset[:]\n",
    "print(\"Data sparsity:\", words.to(dtype = torch.float32).mean())\n",
    "del words\n",
    "del context\n",
    "train_dataset[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how many words to use from context_counts\n",
    "num_used_words = len(used_words)\n",
    "\n",
    "def get_model():\n",
    "\n",
    "    word_vecs = torch.nn.Linear(\n",
    "        in_features=num_used_words,\n",
    "        out_features=4,\n",
    "        bias=False,\n",
    "        device=device,\n",
    "        dtype=dtype\n",
    "    )\n",
    "\n",
    "    # standard initialisation\n",
    "    torch.nn.init.constant_(word_vecs.weight, 1/torch.numel(word_vecs.weight))\n",
    "\n",
    "    return torch.nn.Sequential(\n",
    "        word_vecs,\n",
    "        torch.nn.ReLU(),\n",
    "        torch.nn.Linear(\n",
    "            in_features=4,\n",
    "            out_features=2,\n",
    "            bias=False,\n",
    "            device=device,\n",
    "            dtype=dtype\n",
    "        ),\n",
    "        torch.nn.LogSoftmax(dim=1)\n",
    "    )\n",
    "\n",
    "model = get_model()\n",
    "\n",
    "print(next(model[0].parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "batch_size = 20\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "print(next(iter(train_dataloader)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words, context = train_dataset[:5]\n",
    "pred = model(words.to(dtype = torch.float32))\n",
    "print(pred)\n",
    "print(context)\n",
    "predicted_class = pred.argmax(dim=1)\n",
    "print(f\"{predicted_class=}\")\n",
    "nlll = torch.nn.NLLLoss()\n",
    "print(nlll(pred, context))\n",
    "-pred[range(len(context)), context].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The idea here is to train the vectors for the words as the weight matrix\n",
    "of the first layer. The matrix would get updated according to the loss,\n",
    "so that a vector that matches a negative word should result in a more\n",
    "negative guess, and similarly for positive words. Further, words that\n",
    "appear together get updated similarly.\n",
    "\n",
    "Not sure whether these should be updated in batches or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_model()\n",
    "loss_fn = torch.nn.NLLLoss()\n",
    "optim = torch.optim.Adam(model.parameters(), lr = 1e-3)\n",
    "\n",
    "\n",
    "losses = []\n",
    "train_accs = []\n",
    "eval_accs = []\n",
    "\n",
    "def test(model, test_data):\n",
    "\n",
    "    words, context = test_data[:]\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "\n",
    "        pred = model(words).argmax(dim=1)\n",
    "        \n",
    "        acc = 1-torch.abs(context-pred).mean(dtype=float)\n",
    "\n",
    "\n",
    "    model.train()\n",
    "    return acc.item()\n",
    "\n",
    "total_epochs = 5\n",
    "for epoch in range(total_epochs):\n",
    "\n",
    "    for words, context in train_dataloader:\n",
    "\n",
    "        pred = model(words)\n",
    "        # print(pred)\n",
    "        loss = loss_fn(pred, context)\n",
    "\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "        optim.zero_grad()\n",
    "\n",
    "        losses.append(loss.item())\n",
    "\n",
    "    train_acc = test(model, train_dataset)\n",
    "    print(f\"{train_acc=}\")\n",
    "    train_accs.append(train_acc)\n",
    "\n",
    "    eval_acc = test(model, eval_dataset)\n",
    "    print(f\"{eval_acc=}\")\n",
    "    eval_accs.append(eval_acc)\n",
    "\n",
    "\n",
    "losses = torch.tensor(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import itertools\n",
    "\n",
    "print(losses.size())\n",
    "\n",
    "window = 50\n",
    "rolling_average_loss = list(\n",
    "    map(lambda val: sum(val)/window, itertools.batched(losses, window))\n",
    ")\n",
    "print(f\"Mean: {losses.mean().item()}\")\n",
    "print(f\"Var: {losses.var().item()}\")\n",
    "print(f\"{eval_acc=}\")\n",
    "plt.plot(\n",
    "    rolling_average_loss,\n",
    ")\n",
    "_ = plt.title(f\"Rolling average loss, window: {window}\\n epochs: {total_epochs} data per epoch: {len(train_dataset)} batch size: {batch_size}\")\n",
    "# plt.plot(sorted(preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(enumerate(used_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get vector representations of the words from the model\n",
    "word_vecs = next(iter(model.parameters())).T.clone().detach()\n",
    "chosen_idx = 19\n",
    "# vector representation of the word\n",
    "chosen_word = word_vecs[chosen_idx]\n",
    "# actual word itself\n",
    "print(\"Chosen word:\", used_words[chosen_idx])\n",
    "\n",
    "# determine how similar each of the other words is to this one\n",
    "cos = torch.nn.CosineSimilarity(dim=-1)\n",
    "similarities = torch.tensor([cos(chosen_word, word_vecs[i]) for i in range(len(word_vecs))])\n",
    "most_similar = torch.argsort(similarities, descending=True)\n",
    "plt.plot(similarities.flatten().sort()[0])\n",
    "print(*used_words[most_similar.numpy()].to_list(), sep=\"\\n\", file=open(\"temp.dat\", \"w\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "words, context = test_dataset[:]\n",
    "model.eval()\n",
    "pred = model(words).detach()\n",
    "print(pred)\n",
    "print(torch.exp(pred))\n",
    "print(context)\n",
    "acc = 1-torch.abs(context - pred.argmax(dim=1)).mean(dtype=float)\n",
    "print(f\"{acc=}\")\n",
    "pred = torch.exp(pred)\n",
    "# how far the predictions were from the true value\n",
    "surprise = 1-torch.gather(pred, 1, context.reshape((-1,1)))\n",
    "surprise = surprise.flatten().to(device=\"cpu\")\n",
    "print(surprise)\n",
    "\n",
    "surprise_mean = surprise.mean(dtype=float)\n",
    "print(f\"Mean of surprises: {surprise_mean}\")\n",
    "plt.hist(surprise)\n",
    "plt.title(\"Distribution of surprises\")\n",
    "plt.figure()\n",
    "plt.plot(surprise.sort().values)\n",
    "plt.title(\"Sorted surprises\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
