{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "\n",
    "%autoreload 2\n",
    "\n",
    "from nltk.stem.snowball import EnglishStemmer\n",
    "from nltk.corpus import stopwords\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "from src.string_processing import StringProcessor\n",
    "from src.context_counter import ContextCounter\n",
    "# import nltk\n",
    "# nltk.download('stopwords')\n",
    "# processor = StringProcessor(\n",
    "#         EnglishStemmer().stem,\n",
    "#         stopwords.words(\"english\")\n",
    "# )\n",
    "\n",
    "# counter = ContextCounter()\n",
    "\n",
    "data_path = Path() / \"data\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "raw_context_counts = pd.read_csv(data_path / \"train_context_counts.csv\")\n",
    "# the contents of \"train_context_counts.csv\" should be similar to (\n",
    "# based on the IMDB Dataset):\n",
    "# word,negative,positive,total\n",
    "# movi,29416,22491,51907\n",
    "# film,22487,25661,48148\n",
    "# one,13555,14103,27658\n",
    "\n",
    "def ready_context_counts(context_counts):\n",
    "    '''\n",
    "    Calculate some values from the existing data in <context_counts>.\n",
    "    '''\n",
    "\n",
    "    context_counts.set_index(\"word\", inplace=True)\n",
    "    context_counts[\"score\"] = (\n",
    "        context_counts[\"positive\"] \n",
    "        - context_counts[\"negative\"]\n",
    "    )/context_counts[\"total\"]\n",
    "    context_counts[\"pos_prob\"] = (\n",
    "        context_counts[\"total\"]\n",
    "        - context_counts[\"negative\"]\n",
    "    )/context_counts[\"total\"]\n",
    "    \n",
    "    context_counts[\"weight\"] = np.log2(\n",
    "        context_counts[\"total\"]/context_counts[\"total\"].min()+1\n",
    "    )\n",
    "    return context_counts\n",
    "\n",
    "context_counts = ready_context_counts(\n",
    "    raw_context_counts.query(\"total > 1000\", inplace=False)\n",
    ")\n",
    "\n",
    "# only take values that have a good \"confidence\"\n",
    "context_counts = context_counts[context_counts[\"score\"].abs() > 0.10]\n",
    "\n",
    "# read in the context and processed words for some test data\n",
    "# assumes test_data file contains a file one example\n",
    "# per line. Each example has context first, then words.\n",
    "# \"context\" and \"words\" are separated by a comma, words are separated\n",
    "# spaces\n",
    "test_data = []\n",
    "with open(data_path / \"test_data.csv\", encoding=\"utf-8\") as f:\n",
    "\n",
    "    for i, row in enumerate(f.readlines()):\n",
    "        # skip header line\n",
    "        if i == 0:\n",
    "            continue\n",
    "        context, words = row.strip().split(\",\")\n",
    "        test_data.append({\"context\":context, \"words\":words.split()})\n",
    "\n",
    "print(*test_data[:3], sep = \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "context_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_text(\n",
    "        context_counts:pd.DataFrame,\n",
    "        words:list,\n",
    "        score_funcs:list\n",
    ") -> list[float]:\n",
    "    '''\n",
    "    Filter `context_counts` based on `words` then use `score_funcs`\n",
    "    to calulate the scores for the found words.\n",
    "    '''\n",
    "\n",
    "    filtered = context_counts.filter(words, axis=\"index\")\n",
    "    return [score_func(filtered) for score_func in score_funcs]\n",
    "\n",
    "def weighted_score(context_counts:pd.DataFrame):\n",
    "\n",
    "\n",
    "    scores = context_counts[\"score\"]\n",
    "    weights = context_counts[\"weight\"]\n",
    "\n",
    "    return (scores*weights).sum()/weights.sum()\n",
    "\n",
    "\n",
    "def basic_score(context_counts:pd.DataFrame):\n",
    "\n",
    "    return context_counts[\"score\"].sum()\n",
    "\n",
    "\n",
    "def prob_score(context_counts:pd.DataFrame):\n",
    "\n",
    "    return context_counts[\"pos_prob\"].mean()\n",
    "\n",
    "def weighted_prob_score(context_counts:pd.DataFrame):\n",
    "\n",
    "    scores = context_counts[\"pos_prob\"]\n",
    "    weights = context_counts[\"weight\"]\n",
    "\n",
    "    return (scores*weights).sum()/weights.sum()\n",
    "\n",
    "def positive(context, score):\n",
    "\n",
    "    return bool(\n",
    "        (context == \"negative\" and score < 0)\n",
    "        or (context == \"positive\" and score >= 0)\n",
    "    )\n",
    "\n",
    "def greater_than_half(context, score):\n",
    "\n",
    "    return bool(\n",
    "        (context == \"negative\" and score < 0.5)\n",
    "        or (context == \"positive\" and score >= 0.5)\n",
    "    )\n",
    "\n",
    "def pn_accuracy(pn_rate):\n",
    "        '''\n",
    "        Calculate the accuracy of the model.\n",
    "        '''\n",
    "\n",
    "        return pn_rate[True].total()/(pn_rate[True].total()+pn_rate[False].total())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test some different scoring methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%script skip_this_one\n",
    "\n",
    "score_funcs = [prob_score, weighted_prob_score]\n",
    "# these compare the score given by the model to the actual, true\n",
    "# context of the test set data point\n",
    "prediction_funcs = [greater_than_half, greater_than_half]\n",
    "used_scores = len(score_funcs)\n",
    "\n",
    "saved_context_counts = context_counts.copy()\n",
    "\n",
    "pn_df = pd.DataFrame(dict(\n",
    "    training_words_num = [],    \n",
    ") | {fu.__name__: [] for fu in score_funcs})\n",
    "\n",
    "# test performance with different number of keywords from the training set\n",
    "training_words_num = []\n",
    "pn_accuracies = []\n",
    "for j in range(0,len(context_counts),len(context_counts)//10):\n",
    "\n",
    "    if j == 0:\n",
    "        continue\n",
    "    training_words_num.append(j)\n",
    "\n",
    "    print(f\"Testing predictions with {j} words\")\n",
    "\n",
    "    # take <j> most popular words found in training data\n",
    "    context_counts = saved_context_counts.iloc[:j,:]\n",
    "\n",
    "    pn_rates = [ContextCounter() for _ in range(used_scores)]\n",
    "    \n",
    "    # TODO: test_data could use some class definition for a standardised\n",
    "    # interface\n",
    "    for data_point in test_data[:2000]:\n",
    "\n",
    "        scores = score_text(\n",
    "            context_counts, \n",
    "            data_point[\"words\"], \n",
    "            score_funcs\n",
    "        )\n",
    "        data_point[\"score\"] = scores\n",
    "\n",
    "        # make predictions, calculate positive-negative -table\n",
    "        context = data_point[\"context\"]\n",
    "        data_point[\"correct_prediction\"] = [None]*used_scores\n",
    "        for i, score in enumerate(scores):\n",
    "\n",
    "            correct_prediction = prediction_funcs[i](context, score)\n",
    "\n",
    "            data_point[\"correct_prediction\"][i] = correct_prediction\n",
    "\n",
    "            pn_rates[i].add(context, [correct_prediction])\n",
    "\n",
    "\n",
    "    pn_df.loc[len(pn_df),:] = [j] + [pn_accuracy(pn_rate) for pn_rate in pn_rates] \n",
    "\n",
    "context_counts = saved_context_counts.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "title = '''\\\n",
    "    Accuracy of different scoring functions as a function of the number\\n\\\n",
    "    of training words used in predicting\\\n",
    "'''\n",
    "\n",
    "pn_df.plot(\n",
    "    x=\"training_words_num\",\n",
    "    y=[\"prob_score\", \"weighted_prob_score\"],\n",
    "    title=title,\n",
    "    xlabel=\"Number of words used to predict\",\n",
    "    ylabel=\"Accuracy\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pn_rate = ContextCounter()\n",
    "for data_point in test_data:\n",
    "    score = score_text(context_counts, data_point[\"words\"], [prob_score])[0]\n",
    "    context = data_point[\"context\"]\n",
    "    pn_rate.add(context,[greater_than_half(context, score)])\n",
    "\n",
    "print(pn_accuracy(pn_rate))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
