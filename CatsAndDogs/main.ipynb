{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read from Kaggle, unzip file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload\n",
    "\n",
    "import urllib.request as urlrequest\n",
    "from pathlib import Path\n",
    "\n",
    "base_data_path = Path() / \"data\" \n",
    "\n",
    "base_data_path.mkdir(parents = True, exist_ok = True)\n",
    "\n",
    "dataset_location = base_data_path / \"dataset\"\n",
    "\n",
    "zip_path = base_data_path / \"dataset.zip\"\n",
    "\n",
    "if not zip_path.exists():\n",
    "\n",
    "    dataset_url = \"https://www.kaggle.com/api/v1/datasets/download/alvarole/asirra-cats-vs-dogs-object-detection-dataset\"\n",
    "\n",
    "    response = urlrequest.urlopen(\n",
    "        dataset_url,\n",
    "    )\n",
    "\n",
    "    download_size = response.getheader(\"Content-Length\")\n",
    "    with open(zip_path, \"wb\") as f:\n",
    "\n",
    "        f.write(response.read())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import zipfile\n",
    "import os\n",
    "\n",
    "def unzip():\n",
    "\n",
    "    inner_file = \"Asirra_ cat vs dogs\"\n",
    "    with zipfile.ZipFile(zip_path, \"r\") as zip:\n",
    "        \n",
    "        for item in zip.infolist():\n",
    "\n",
    "            zip.extract(item, base_data_path)\n",
    "\n",
    "    os.rename(base_data_path / inner_file, dataset_location)\n",
    "\n",
    "# unzip()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import itertools\n",
    "import xml.etree.ElementTree as ET\n",
    "import torch\n",
    "from typing import TypedDict\n",
    "\n",
    "\n",
    "def patched_dataset_paths(dataset_location):\n",
    "\n",
    "    return itertools.batched(dataset_location.iterdir(), 2)\n",
    "\n",
    "class Objects(TypedDict):\n",
    "    '''\n",
    "    `bndbox`: (xmin,ymin,xmax,ymax)\n",
    "    '''\n",
    "\n",
    "    name: str\n",
    "    pose: str\n",
    "    truncated: int\n",
    "    difficult: int\n",
    "    bndbox: torch.Tensor\n",
    "\n",
    "class Metadata(TypedDict):\n",
    "    '''\n",
    "    `size`: (width, height, depth)\n",
    "    '''\n",
    "\n",
    "    size: torch.Tensor\n",
    "    objects: list[Objects]\n",
    "\n",
    "class MetaWithImage(Metadata):\n",
    "\n",
    "    img_path: str\n",
    "\n",
    "# specific xml reader implementation for the lolz\n",
    "def read_metadata(xml_file: Path) -> Metadata:\n",
    "    '''\n",
    "    Read labeling from xml file into dict.\n",
    "    '''\n",
    "\n",
    "    with open(xml_file, \"r\", encoding = \"utf-8\") as f:\n",
    "        text = ET.canonicalize(from_file=f, strip_text = True)\n",
    "        \n",
    "    tree = ET.fromstring(text)\n",
    "\n",
    "    size = tree.find(\"size\")\n",
    "    size = torch.tensor([int(elem.text) for elem in size.iter() if not elem.tag == \"size\"])\n",
    "\n",
    "\n",
    "    objects = tree.findall(\"object\")\n",
    "    objects: Objects = [dict(\n",
    "        name = obj.find(\"name\").text,\n",
    "        pose = obj.find(\"pose\").text,\n",
    "        truncated = int(obj.find(\"truncated\").text),\n",
    "        difficult = int(obj.find(\"difficult\").text),\n",
    "        bndbox = torch.tensor([\n",
    "            float(elem.text) \n",
    "            for elem in obj.find(\"bndbox\").iter()\n",
    "            if not elem.tag == \"bndbox\"\n",
    "        ])\n",
    "    ) for obj in objects]\n",
    "\n",
    "    metadata: Metadata = dict(\n",
    "        size = size,\n",
    "        objects = objects\n",
    "    )\n",
    "\n",
    "    return metadata\n",
    "\n",
    "def get_dataset(dataset_location) -> list[MetaWithImage]:\n",
    "\n",
    "    meta: list[MetaWithImage] = []\n",
    "    for img, xml_path in patched_dataset_paths(dataset_location):\n",
    "\n",
    "        metadata: MetaWithImage = read_metadata(xml_path) | dict(img_path = img)\n",
    "        meta.append(metadata)\n",
    "\n",
    "    return meta\n",
    "\n",
    "def dataset_splits(dataset: list[MetaWithImage] | None = None, fractions: tuple[float] = (0.8, 0.1, 0.1)):\n",
    "\n",
    "    dataset = get_dataset(dataset_location) if dataset is None else dataset\n",
    "    return  torch.utils.data.random_split(dataset, fractions)\n",
    "\n",
    "\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision.io import decode_image\n",
    "import torchvision.transforms.v2.functional as tvt\n",
    "\n",
    "class CatsAndDogsDataset(torch.utils.data.Dataset):\n",
    "    '''\n",
    "    `class_dict`: dict\n",
    "    - \"background\" matches zero, other values in `class_names` (passed\n",
    "    to init) match subsequent indices incrementally.\n",
    "    '''\n",
    "\n",
    "    def __init__(self, data: list[MetaWithImage], class_names: list[str], resize_to = (300,300)):\n",
    "        '''\n",
    "\n",
    "        `class_names`:\n",
    "        - names of classes in dataset. \n",
    "\n",
    "        `resize_to`:\n",
    "        - should be square\n",
    "        '''\n",
    "\n",
    "        classes = [\"background\"] + class_names\n",
    "        self.class_dict = {classes[i]: i for i in range(len(classes))}\n",
    "        self.resize_to = resize_to\n",
    "        self.data = [self.metadata_transform(val) for val in data]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def image_transform(self, img):\n",
    "\n",
    "        return tvt.resize(img, self.resize_to)\n",
    "\n",
    "    def metadata_transform(self, metadata: MetaWithImage):\n",
    "        '''\n",
    "        Transform the bndbox values to be in the range [0,1],\n",
    "        and change the \"objects\" contents to be a dict\n",
    "        { bndbox: torch.Tensor, class: torch.Tensor }, where `bndbox` is\n",
    "        (N,4) and `class` (N). \n",
    "        '''\n",
    "\n",
    "        resize_x, resize_y = self.resize_to\n",
    "        width, height, depth = metadata['size']\n",
    "\n",
    "        bndboxes = []\n",
    "        classes = []\n",
    "\n",
    "        for i in range(len(metadata[\"objects\"])):\n",
    "            obj = metadata['objects'][i]\n",
    "            bndbox = obj['bndbox']\n",
    "            bndboxes.append(\n",
    "                bndbox/torch.tensor([width, height]*2, dtype=torch.float32)\n",
    "            )\n",
    "\n",
    "            classes.append(self.class_dict[obj[\"name\"]])\n",
    "\n",
    "        metadata[\"size\"] = torch.tensor([resize_x, resize_y, depth])\n",
    "        metadata[\"objects\"] = {\n",
    "            \"bndbox\": torch.vstack(bndboxes),\n",
    "            \"class\": torch.tensor(classes, dtype=int)\n",
    "        }\n",
    "\n",
    "        return metadata\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        metadata = self.data[idx]\n",
    "        img_path = metadata[\"img_path\"]\n",
    "        image = tvt.to_dtype(decode_image(img_path), scale = True)\n",
    "        image = self.image_transform(image)\n",
    "        return image, metadata\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_split, validation_split, test_split = map(\n",
    "    lambda split: CatsAndDogsDataset(split, [\"cat\", \"dog\"]),\n",
    "    dataset_splits()\n",
    ")\n",
    "\n",
    "print(len(train_split))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test datasets with plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import torchvision.transforms.v2.functional as vision_transforms\n",
    "from torchvision.utils import draw_bounding_boxes\n",
    "\n",
    "def to_plottable(img):\n",
    "\n",
    "    return vision_transforms.to_pil_image(img) \n",
    "\n",
    "def add_bb(img, bndbox: torch.Tensor, color = \"cyan\"):\n",
    "    '''\n",
    "    `bdnbox`:\n",
    "\n",
    "    (N, 4) with (xmin,ymin,xmax,ymax) (relative to dimensions of image).\n",
    "    '''\n",
    "\n",
    "    width, height, _ = [val.item() for val in meta[\"size\"]]\n",
    "    bb = bndbox*torch.tensor([width, height, width, height])\n",
    "    img = draw_bounding_boxes(img, bb, colors = color)\n",
    "\n",
    "    return img\n",
    "\n",
    "plt.figure()\n",
    "im, meta = train_split[int(torch.rand(1).item()*len(train_split))]\n",
    "im = add_bb(im, meta[\"objects\"][\"bndbox\"])\n",
    "plt.imshow(to_plottable(im))\n",
    "print(meta[\"objects\"][\"bndbox\"])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test IoU calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import src.default_box as default_box\n",
    "import src.utils.math as math_utils\n",
    "\n",
    "\n",
    "im, meta = train_split[0]\n",
    "bndbox = meta[\"objects\"][\"bndbox\"]\n",
    "width, height, _ = [val.item() for val in meta[\"size\"]]\n",
    "\n",
    "print(width, height)\n",
    "boxes = default_box.default_boxes(\n",
    "    scale = 0.8,\n",
    "    centers = default_box.default_box_centers(width//8, height//8)\n",
    ")\n",
    "\n",
    "num_boxes, num_ratios, _ = boxes.shape\n",
    "boxes = boxes.reshape([num_boxes*num_ratios, 4])\n",
    "\n",
    "print(bndbox)\n",
    "print(boxes.shape)\n",
    "iou = math_utils.intersection_over_union(\n",
    "    boxes,\n",
    "    bndbox\n",
    ")\n",
    "print(iou.shape)\n",
    "iou = iou.reshape([num_ratios, len(bndbox), num_boxes])\n",
    "print(iou.shape)\n",
    "\n",
    "print([(iou >= val).sum() for val in [0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9]])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Design network and training setup\n",
    "\n",
    "See [SSD paper](https://arxiv.org/abs/1512.02325), also [Dive into deep learning](https://d2l.ai/) has good practical material and examples.\n",
    "\n",
    "SSD consists of a base network followed by successive prediction layers\n",
    "which generate the class predictions and bounding box offsets at different\n",
    "scales. \n",
    "\n",
    "- The base network downsamples the input, decreasing the width and height\n",
    "while adding more channels.\n",
    "\n",
    "- The prediction layers make the predictions\n",
    "by convolving over their input and outputting a value for each class\n",
    "and each default box offset. \n",
    "    * The output from a layer is further downsampled by e.g. pooling,\n",
    "    creating a larger receptive field for the next layer. As a result,\n",
    "    the scale of default boxes should increase further into the net.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from math import floor\n",
    "\n",
    "import src.utils.reshape as reshape\n",
    "\n",
    "def down_sampler(in_channels, out_channels, device = None):\n",
    "\n",
    "    if device is None:\n",
    "        device = torch.get_default_device()\n",
    "\n",
    "    return torch.nn.Sequential(*[\n",
    "        torch.nn.Conv2d(\n",
    "            in_channels = in_channels,\n",
    "            out_channels = out_channels,\n",
    "            kernel_size = 3,\n",
    "            padding = 1,\n",
    "            device = device\n",
    "        ),\n",
    "        torch.nn.ReLU(),\n",
    "        torch.nn.MaxPool2d(kernel_size = 2)\n",
    "    ])\n",
    "\n",
    "def prediction_layer(in_channels, num_classes, num_ratios, device = None):\n",
    "\n",
    "    if device is None:\n",
    "        device = torch.get_default_device()\n",
    "\n",
    "    return torch.nn.ModuleDict(\n",
    "        dict(\n",
    "            class_pred = torch.nn.Conv2d(\n",
    "                in_channels = in_channels,\n",
    "                out_channels = num_classes*num_ratios,\n",
    "                kernel_size = 3,\n",
    "                padding = 1,\n",
    "                device = device\n",
    "            ),\n",
    "            box_pred = torch.nn.Conv2d(\n",
    "                in_channels = in_channels,\n",
    "                out_channels = 4*num_ratios,\n",
    "                kernel_size = 3,\n",
    "                padding = 1,\n",
    "                device = device\n",
    "            )\n",
    "        )\n",
    "    )\n",
    "\n",
    "def max_pool_change(in_size):\n",
    "    '''\n",
    "    How much an input size of `in_size` changes using\n",
    "    torch.nn.MaxPool2D(kernel_size = 2)\n",
    "    '''\n",
    "            \n",
    "    return floor((in_size - (2-1) - 1)/2 + 1)\n",
    "\n",
    "def repeat_apply(func, _input, num):\n",
    "\n",
    "    for _ in range(num):\n",
    "        _input = func(_input)\n",
    "\n",
    "    return _input\n",
    "\n",
    "def generate_default_boxes(width, scale):\n",
    "\n",
    "    return default_box.default_boxes(\n",
    "        scale = scale,\n",
    "        centers = default_box.default_box_centers(width, width)\n",
    "    )\n",
    "\n",
    "class SSD(torch.nn.Module):\n",
    "    '''\n",
    "\n",
    "    `num_classes`: int\n",
    "    \n",
    "    `num_ratios`: int\n",
    "    - Number of ratios used for the default boxes.\n",
    "\n",
    "    `default_boxes`: torch.Tensor\n",
    "    - default boxes for each feature map layer, of shape\n",
    "    (num_ratios*feature_map_height*feature_map_width,4). Indexing\n",
    "    over boxes per pixel works by [num_ratios*i:num_ratios*(i+1), 4]\n",
    "\n",
    "    `pixels_per_layer`: list of int\n",
    "    - pixels per feature map layer, useful for iterating over `default_boxes`\n",
    "    in layer-by-layer manner.\n",
    "    '''\n",
    "\n",
    "    def __init__(self, num_classes, num_ratios, in_channels = 3, width = 300, device = None):\n",
    "        '''\n",
    "        `num_classes`:\n",
    "        - Number of classes including background \"class\".\n",
    "        '''\n",
    "\n",
    "        if device is None:\n",
    "            device = torch.get_default_device()\n",
    "\n",
    "        self.device = device\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        base_channels = [in_channels,9,27,81]\n",
    "\n",
    "        self.num_classes = num_classes\n",
    "        self.num_ratios = num_ratios\n",
    "\n",
    "        self.max_pool = torch.nn.MaxPool2d(kernel_size = 2)\n",
    "\n",
    "        self.base_layers = torch.nn.ModuleDict({\n",
    "            \"box\": torch.nn.Sequential(*[\n",
    "                down_sampler(base_channels[i], base_channels[i+1], device = device)\n",
    "                for i in range(len(base_channels)-1)\n",
    "            ]),\n",
    "            \"class\": torch.nn.Sequential(*[\n",
    "                down_sampler(base_channels[i], base_channels[i+1], device = device)\n",
    "                for i in range(len(base_channels)-1)\n",
    "            ])\n",
    "        })\n",
    "\n",
    "        self.prediction_layers = [\n",
    "            prediction_layer(base_channels[-1], num_classes, num_ratios, device = device)\n",
    "            for _ in range(3)\n",
    "        ]\n",
    "        \n",
    "        # pre-generate the default boxes\n",
    "        # ------------------------------\n",
    "        start_size = repeat_apply(max_pool_change, width, len(base_channels)-1)\n",
    "        \n",
    "        scales = default_box.scales(len(self.prediction_layers))\n",
    "\n",
    "        def_boxes = []\n",
    "        self.pixels_per_layer = []\n",
    "\n",
    "        for i in range(len(self.prediction_layers)):\n",
    "\n",
    "\n",
    "            def_box = generate_default_boxes(start_size, scales[i])\n",
    "            pixels = def_box.shape[0]\n",
    "            self.pixels_per_layer.append(pixels)\n",
    "            # shape to match the output from the box predictions:\n",
    "            # [pixels*ratios, boxes]\n",
    "            def_box = (\n",
    "                def_box\n",
    "                .flatten(start_dim = 1)\n",
    "                .reshape([pixels*num_ratios, 4])\n",
    "            )\n",
    "            def_boxes.append(\n",
    "                def_box\n",
    "            )\n",
    "\n",
    "            start_size = max_pool_change(start_size)\n",
    "\n",
    "        self.default_boxes = torch.vstack(def_boxes)\n",
    "        # ===============================\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        '''\n",
    "        Return dict with kwords class_preds, box_preds,\n",
    "        both a list of torch.Tensor of predictions per feature map layer.\n",
    "        '''\n",
    "\n",
    "        x = x.to(device = self.device)\n",
    "        X_box = self.base_layers[\"box\"](x)\n",
    "        X_class = self.base_layers[\"class\"](x)\n",
    "\n",
    "        pl = self.prediction_layers\n",
    "\n",
    "        class_preds = []\n",
    "        box_preds = []\n",
    "        for i in range(len(pl)):\n",
    "\n",
    "            # predict classes \n",
    "            # (reshape to have predicted classes per default box)\n",
    "            class_pred = reshape.items_per_pixel(\n",
    "                pl[i][\"class_pred\"](X_class),\n",
    "                self.num_classes\n",
    "            )\n",
    "            class_preds.append(\n",
    "                torch.nn.functional.log_softmax(class_pred, dim=2).to(\"cpu\")\n",
    "                \n",
    "            )\n",
    "\n",
    "            # predict box offsets\n",
    "            box_pred = reshape.items_per_pixel(pl[i][\"box_pred\"](X_box), 4)\n",
    "            # restrict w/h offset to > 0 (w/h used to scale width and height)\n",
    "            box_pred[:,:,[2,3]] = torch.nn.functional.softplus(box_pred[:,:,[2,3]])\n",
    "\n",
    "            box_preds.append(\n",
    "                box_pred.to(\"cpu\")\n",
    "            )\n",
    "\n",
    "            X_box = self.max_pool(X_box)\n",
    "            X_class = self.max_pool(X_class)\n",
    "\n",
    "        return dict(\n",
    "            class_preds = class_preds,\n",
    "            box_preds = box_preds\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import accumulate\n",
    "import src.utils.torch_util as torch_utils\n",
    "\n",
    "\n",
    "dev = \"cpu\"\n",
    "dev = torch_utils.TORCH_DEVICE\n",
    "model = SSD(3, 5, device = dev)\n",
    "\n",
    "# test properties\n",
    "print(model.default_boxes.shape)\n",
    "print(model.pixels_per_layer)\n",
    "\n",
    "img1 = train_split[0][0]\n",
    "img2 = train_split[1][0]\n",
    "img_stack = torch.stack([img1, img2])\n",
    "\n",
    "res = model(img_stack)\n",
    "print(res[\"class_preds\"][0].shape)\n",
    "print(f\"{torch.hstack(res[\"class_preds\"]).shape=}\")\n",
    "\n",
    "bpreds = res[\"box_preds\"][0]\n",
    "print(res[\"box_preds\"][0].shape)\n",
    "\n",
    "print(model.default_boxes[:5,:])\n",
    "\n",
    "# should match in dimensions\n",
    "box_preds = torch.hstack(res[\"box_preds\"])\n",
    "def_boxes = model.default_boxes\n",
    "\n",
    "box_sum = model.default_boxes + box_preds\n",
    "print(f\"{box_sum.shape=}\")\n",
    "\n",
    "# how to go over feature layers\n",
    "layer_indices = list(accumulate(model.pixels_per_layer))\n",
    "print(layer_indices)\n",
    "\n",
    "assert torch.all(\n",
    "    torch.tensor(box_sum[0][layer_indices[-2]*model.num_ratios:,:].shape) \n",
    "    == torch.tensor([model.pixels_per_layer[-1]*model.num_ratios, 4])\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utilities for calculating loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# print(train_split[0])\n",
    "\n",
    "\n",
    "dat = train_split[0][1]\n",
    "bndbox, clslist = dat[\"objects\"].values()\n",
    "\n",
    "def classes_and_boxes_truth(\n",
    "    iou: torch.Tensor,\n",
    "    ground_truth_classes: torch.Tensor,\n",
    "    threshold = 0.5,\n",
    ") -> tuple[torch.Tensor, torch.Tensor]:\n",
    "    '''\n",
    "    Calculate a tensor indicating which default box is considered\n",
    "    to overlap which class and which ground truth box.\n",
    "\n",
    "    Return shape is (number of default boxes), with each element\n",
    "    a class indicator index (background is zero) or box index (-1 is\n",
    "    no box).\n",
    "\n",
    "    `iou`:\n",
    "    - The intersection-over-union of default boxes and ground truth\n",
    "    boxes, as per\n",
    "    \n",
    "    > utils.math.intersection_over_union(model.default_boxes, ground_truth_boxes)\n",
    "    '''\n",
    "\n",
    "    matches_max = iou.max(dim=0)\n",
    "    boxes = matches_max.indices\n",
    "    classes = ground_truth_classes[boxes]\n",
    "    background = matches_max.values <= threshold\n",
    "    boxes[background] = -1\n",
    "    classes[background] = 0\n",
    "    return classes, boxes\n",
    "\n",
    "iou = math_utils.intersection_over_union(\n",
    "    model.default_boxes,\n",
    "    bndbox\n",
    ")\n",
    "classes, boxes = classes_and_boxes_truth(iou, clslist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(iou.shape)\n",
    "(iou > 0.5).sum()\n",
    "\n",
    "class_preds = torch.hstack(res[\"class_preds\"]).to(\"cpu\")\n",
    "\n",
    "print(class_preds.shape)\n",
    "\n",
    "print(classes.shape)\n",
    "print(classes.sum())\n",
    "print((classes == 0).sum())\n",
    "print((classes == 1).sum())\n",
    "print((classes == 2).sum())\n",
    "\n",
    "torch.nn.functional.nll_loss(class_preds[0], classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# matching predicted boxes to ground truth.\n",
    "# - Only calculate for boxes which have a matching ground truth?\n",
    "# - Only calculate for boxes which have a predicted class other than background?\n",
    "#   - Or should box predictions and class predictions be considered\n",
    "#   independent of each other? Is the box prediction layer expected\n",
    "#   to learn to match the ground truth box independent of what the\n",
    "#   target class actually is? Of course, the two are modeled as\n",
    "#   separate, so independence should be assumed, I guess.\n",
    "\n",
    "def offset_default_boxes(\n",
    "    default_boxes: torch.Tensor,\n",
    "    predicted_offsets: torch.Tensor,\n",
    "):\n",
    "    '''\n",
    "    Offset `default_boxes` based on `predicted_offsets`.\n",
    "    '''\n",
    "\n",
    "    predicted = torch.clone(default_boxes).detach()\n",
    "    # move boxes\n",
    "    predicted += predicted_offsets[:, [0,1,0,1]]\n",
    "    # scale boxes\n",
    "    width_height_scaled = (\n",
    "        (predicted[:,[2,3]] - predicted[:,[0,1]])\n",
    "        *predicted_offsets[:, [2,3]]\n",
    "    )\n",
    "    predicted[:,[2,3]] = predicted[:,[0,1]]+width_height_scaled\n",
    "\n",
    "    return predicted\n",
    "\n",
    "def calculate_box_loss(\n",
    "    default_boxes: torch.Tensor,\n",
    "    predicted_offsets: torch.Tensor,\n",
    "    ground_truth_boxes: torch.Tensor,\n",
    "    ground_truth_overlap_index: torch.Tensor\n",
    "):\n",
    "    '''\n",
    "    Calculate a Smooth L1 Loss between predicted object boxes\n",
    "    and actual ones. \n",
    "    \n",
    "    `default_boxes` (xmin, ymin, xmax, ymax) will\n",
    "    be offset by `predicted_offsets` (x,y,w,h): x/y is used\n",
    "    to move the entire box along an axis, allowing any real value;\n",
    "    w/h >= 0 is used to scale the box, keeping the xmin/ymin stationary\n",
    "    while increasing/decreasing the distance of xmax/ymax from the\n",
    "    former. The result of the offset is compared against the ground\n",
    "    truth box in `ground_truth_boxes` which the default box in question\n",
    "    matched against (see `classes_and_boxes_truth()`) based on \n",
    "    `ground_truth_overlap_index`.\n",
    "    '''\n",
    "\n",
    "    # calculate only for values which had a match\n",
    "    match = ground_truth_overlap_index != -1\n",
    "    predicted = offset_default_boxes(default_boxes[match], predicted_offsets[match])\n",
    "\n",
    "    # match the ground truths to the default boxes based on\n",
    "    # overlap\n",
    "    indices = ground_truth_overlap_index[match]\n",
    "    actual = ground_truth_boxes[indices,:]\n",
    "\n",
    "    return torch.nn.functional.smooth_l1_loss(\n",
    "        input = predicted,\n",
    "        target = actual,\n",
    "    )\n",
    "\n",
    "\n",
    "calculate_box_loss(\n",
    "    model.default_boxes,\n",
    "    box_preds[0],\n",
    "    bndbox,\n",
    "    boxes\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### class losses\n",
    "\n",
    "The losses here are at least supposed to perform the hard negative\n",
    "mining (HNM) suggested for the SSD, namely only using some of the\n",
    "many negative samples to train the model. This is supposed to\n",
    "improve the training, as it emphasises positive examples more.\n",
    "\n",
    "The two ways of calculating (or including) HNM here are to do it\n",
    "1. based on the true values, so pick only a fraction of the true background\n",
    "items for comparison\n",
    "2.  based on the predicted values, so pick only a fraction of the predicted\n",
    "background items for comparison.\n",
    "\n",
    "Not personally sure how they originally exactly did it, but no harm\n",
    "in doing it both ways and summing (or something more complex, perhaps),\n",
    "I figure, besides computational considerations. \n",
    "To reason this, one can consider only using one or the other. \n",
    "\n",
    "1. By only picking values based on the true classes, there's possibly\n",
    "very few false positive examples to compare: by picking a fraction of background indices from the true class\n",
    "set, it's possible that many of the excluded indices would be (falsely)\n",
    "predicted as some positive class. Therefore many of these false positive\n",
    "examples would be lost. We would focus on what we do want to predict\n",
    "as positive, but not as much what we don't.\n",
    "2. By only picking background fractions\n",
    "from the predicted set, many of the true positive examples would potentially be\n",
    "lost, especially at first as the predictions are likely uniformly\n",
    "distributed across the classes.\n",
    "\n",
    "In general, the more each class\n",
    "can be compared with each other, the better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def calculate_class_loss_true(\n",
    "    predicted_classes,\n",
    "    matched_classes,\n",
    "    negative_to_positive_ratio: 3.0\n",
    "):\n",
    "    '''\n",
    "    Calculate an NLL loss between the class predictions in\n",
    "    `predicted_classes`, (N,C) and true class values in `matched_classes`,\n",
    "    (N). `predicted_classes` rows should be log-probabilities for each\n",
    "    class, with the column index matching the set class index (0 being\n",
    "    background).\n",
    "\n",
    "    Potentionally ignores excessive negative examples based on `matched_classes`\n",
    "    and `negative_to_positive_ratio`.\n",
    "\n",
    "    `negative_to_positive_ratio`:\n",
    "    - Determines how many more negative (background) class examples\n",
    "    should used compared to the number of available positive examples.\n",
    "    Lower ratios increase focus on correct positive predictions, \n",
    "    and higher ones increase focus on correct background predictions.\n",
    "    It is assumed that the number of negative examples is much higher,\n",
    "    therefore this will only affect the picking of those examples.\n",
    "    The number of picked negative examples is still between zero\n",
    "    and the total number of negative examples.\n",
    "    '''\n",
    "    \n",
    "    # pick negative/positive examples in a (at most) 3:1 ratio\n",
    "    # --------------------------------------------------------\n",
    "    true_background = matched_classes == 0\n",
    "    num_background = true_background.sum()\n",
    "    true_other = torch.logical_not(true_background)\n",
    "    num_other = true_other.sum()\n",
    "    neg_to_pick = max(\n",
    "        min(num_background, negative_to_positive_ratio*num_other),\n",
    "        0\n",
    "    )\n",
    "    neg_to_pick = int(neg_to_pick)\n",
    "    # randomly get indices of negative examples\n",
    "    choices_idx = torch.argwhere(true_background)\n",
    "    choices_idx = choices_idx[torch.randperm(num_background)[:neg_to_pick]]\n",
    "\n",
    "    # set negative example indices (alongside already existing positive ones)\n",
    "    true_other[choices_idx] = True\n",
    "\n",
    "    # =======================================================\n",
    "    \n",
    "\n",
    "    nll = torch.nn.functional.nll_loss(\n",
    "        predicted_classes[true_other],\n",
    "        matched_classes[true_other],\n",
    "    )\n",
    "\n",
    "    return nll\n",
    "\n",
    "\n",
    "def calculate_class_loss_predicted(\n",
    "    predicted_classes,\n",
    "    matched_classes,\n",
    "    negative_to_positive_ratio: 3.0\n",
    "):\n",
    "    '''\n",
    "    Calculate an NLL loss between the class predictions in\n",
    "    `predicted_classes`, (N,C) and true class values in `matched_classes`,\n",
    "    (N). `predicted_classes` rows should be log-probabilities for each\n",
    "    class, with the column index matching the set class index (0 being\n",
    "    background).\n",
    "\n",
    "    Potentionally ignores excessive negative examples based on `predicted_classes`\n",
    "    and `negative_to_positive_ratio`.\n",
    "\n",
    "    `negative_to_positive_ratio`:\n",
    "    - Determines how many more negative (background) class examples\n",
    "    should used compared to the number of available positive examples.\n",
    "    Lower ratios increase focus on correct positive predictions, \n",
    "    and higher ones increase focus on correct background predictions.\n",
    "    It is assumed that the number of negative examples is much higher,\n",
    "    therefore this will only affect the picking of those examples.\n",
    "    The number of picked negative examples is still between zero\n",
    "    and the total number of negative examples. \n",
    "    '''\n",
    "    \n",
    "    # pick negative/positive examples in a (at most) 3:1 ratio\n",
    "    # --------------------------------------------------------\n",
    "    predicted_background = torch.argmax(predicted_classes, dim=1) == 0\n",
    "    num_background = predicted_background.sum()\n",
    "    predicted_other = torch.logical_not(predicted_background)\n",
    "    num_other = predicted_other.sum()\n",
    "    neg_to_pick = max(\n",
    "        min(num_background, negative_to_positive_ratio*num_other),\n",
    "        0\n",
    "    )\n",
    "    neg_to_pick = int(neg_to_pick)\n",
    "    # randomly get indices of negative examples\n",
    "    choices_idx = torch.argwhere(predicted_background)\n",
    "    choices_idx = choices_idx[torch.randperm(num_background)[:neg_to_pick]]\n",
    "\n",
    "    # set negative example indices (alongside already existing positive ones)\n",
    "    predicted_other[choices_idx] = True\n",
    "\n",
    "    # =======================================================\n",
    "    \n",
    "    nll = torch.nn.functional.nll_loss(\n",
    "        predicted_classes[predicted_other],\n",
    "        matched_classes[predicted_other],\n",
    "    )\n",
    "\n",
    "    return nll\n",
    "\n",
    "\n",
    "def calculate_class_loss(\n",
    "    predicted_classes,\n",
    "    matched_classes,\n",
    "    negative_to_positive_ratio: 3.0\n",
    "):\n",
    "    '''\n",
    "    Calculate an NLL loss between the class predictions in\n",
    "    `predicted_classes`, (N,C) and true class values in `matched_classes`,\n",
    "    (N). `predicted_classes` rows should be log-probabilities for each\n",
    "    class, with the column index matching the set class index (0 being\n",
    "    background).\n",
    "\n",
    "    `negative_to_positive_ratio`:\n",
    "    - Determines how many more negative (background) class examples\n",
    "    should used compared to the number of available positive examples.\n",
    "    Lower ratios increase focus on correct positive predictions, \n",
    "    and higher ones increase focus on correct background predictions.\n",
    "    It is assumed that the number of negative examples is much higher,\n",
    "    therefore this will only affect the picking of those examples.\n",
    "    The number of picked negative examples is still between zero\n",
    "    and the total number of negative examples. \n",
    "    '''\n",
    "    \n",
    "    nll = (\n",
    "        calculate_class_loss_predicted(\n",
    "            predicted_classes,\n",
    "            matched_classes,\n",
    "            negative_to_positive_ratio\n",
    "        )\n",
    "        + calculate_class_loss_true(\n",
    "            predicted_classes,\n",
    "            matched_classes,\n",
    "            negative_to_positive_ratio\n",
    "        )\n",
    "    )\n",
    "\n",
    "    return nll\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def calculate_losses(\n",
    "    default_boxes,\n",
    "    predicted_offsets,\n",
    "    predicted_classes,\n",
    "    ground_truth_boxes,\n",
    "    matched_classes,\n",
    "    ground_truth_overlap_index\n",
    "):\n",
    "    '''\n",
    "    Return dict of l1, nll.\n",
    "    '''\n",
    "    \n",
    "    l1 = calculate_box_loss(\n",
    "        default_boxes,\n",
    "        predicted_offsets,\n",
    "        ground_truth_boxes,\n",
    "        ground_truth_overlap_index\n",
    "    )\n",
    "\n",
    "    nll = calculate_class_loss(predicted_classes,matched_classes, 3.0)\n",
    "\n",
    "    return dict(\n",
    "        l1 = l1,\n",
    "        nll = nll\n",
    "    )\n",
    "\n",
    "calculate_losses(model.default_boxes, box_preds[0], class_preds[0], bndbox, classes, boxes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def calculate_loss(\n",
    "        model: SSD,\n",
    "        prediction,\n",
    "        ground_truth_boxes:list[torch.Tensor],\n",
    "        ground_truth_classes: list[torch.Tensor],\n",
    "        weight: float = 1.0,\n",
    "        iou: list[torch.Tensor | None] | None = None\n",
    "):\n",
    "    '''\n",
    "    Calculate the losses for the output of `model`'s forward pass, `prediction`.\n",
    "    Each batch of `ground_truth_boxes` is (N,4), containing the bounding boxes (xmin, ymin, xmax, ymax) \n",
    "    of the input image objects. Each batch of `ground_truth_classes` is (N), with the elements matching\n",
    "    the true class of the bounding boxes in `ground_truth_boxes`.\n",
    "\n",
    "    Return dict of \"combined\", \"l1\", and \"nll\" over all batches, where \"combined\"\n",
    "    is the combined loss of the other two (including the weight).\n",
    "\n",
    "    `weight` is used as weight for the returned loss as l1 + weight*nll,\n",
    "    where l1 is the loss for the predicted classes, and nll the loss\n",
    "    for the predicted boxes.\n",
    "\n",
    "    `iou` can be passed if it has been calculated using\n",
    "\n",
    "    > iou = src.utils.math.intersection_over_union(model.default_boxes, target_bndbox)\n",
    "    '''\n",
    "\n",
    "    batches = len(ground_truth_boxes)\n",
    "\n",
    "    total_losses = dict(\n",
    "        l1 = 0,\n",
    "        nll = 0\n",
    "    )\n",
    "    class_preds = torch.hstack(prediction[\"class_preds\"])\n",
    "    box_preds = torch.hstack(prediction[\"box_preds\"])\n",
    "    for i in range(batches):\n",
    "\n",
    "        if iou is None or (curr_iou := iou[i]) is None:\n",
    "            curr_iou = math_utils.intersection_over_union(\n",
    "                model.default_boxes,\n",
    "                ground_truth_boxes[i]\n",
    "            )\n",
    "\n",
    "        classes, boxes = classes_and_boxes_truth(curr_iou, ground_truth_classes[i])\n",
    "        losses = calculate_losses(\n",
    "            model.default_boxes,\n",
    "            box_preds[i],\n",
    "            class_preds[i],\n",
    "            ground_truth_boxes[i],\n",
    "            classes,\n",
    "            boxes\n",
    "        )\n",
    "        total_losses[\"l1\"] += losses[\"l1\"]\n",
    "        total_losses[\"nll\"] += losses[\"nll\"]\n",
    "\n",
    "    return dict(\n",
    "        combined = total_losses[\"l1\"] + weight*total_losses[\"nll\"]\n",
    "    ) | total_losses\n",
    "\n",
    "\n",
    "# use same ground truths for testing\n",
    "calculate_loss(model, res, [bndbox]*2, [clslist]*2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import batched\n",
    "from random import sample\n",
    "\n",
    "\n",
    "image_width = 128\n",
    "\n",
    "train_split, test_split, validation_split = map(\n",
    "    lambda split: CatsAndDogsDataset(split, class_names= [\"cat\", \"dog\"], resize_to=(image_width,image_width)),\n",
    "    dataset_splits(fractions = (0.8,0.10,0.10))\n",
    ")\n",
    "\n",
    "# the PyTorch Dataloader seems to do some extra stuff that\n",
    "# doesn't really fit the data from the dataset. Could\n",
    "# probably create a new Dataloader class or maybe\n",
    "# have the dataset return the data somewhat differently,\n",
    "# but doing this for now\n",
    "class Batcher:\n",
    "    '''\n",
    "    Iterator that iterates over given data in batches, possibly randomly,\n",
    "    returning the set of indices and data points for each batch.\n",
    "    '''\n",
    "\n",
    "    def __init__(self, data: list, batch_size: int, shuffle = True):\n",
    "        '''\n",
    "        `shuffle`:\n",
    "        - if False, Don't randomise the order.  \n",
    "        '''\n",
    "\n",
    "        self.idx_and_data = tuple(enumerate(data))\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "        self._iter = None\n",
    "\n",
    "    def __len__(self):\n",
    "\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __iter__(self):\n",
    "\n",
    "        if self.shuffle:\n",
    "            self._iter = batched(\n",
    "                sample(\n",
    "                    self.idx_and_data,\n",
    "                    len(self.idx_and_data)\n",
    "                ),\n",
    "                n = self.batch_size\n",
    "            )\n",
    "        else:\n",
    "            self._iter = batched(\n",
    "                self.idx_and_data,\n",
    "                n = self.batch_size\n",
    "            )\n",
    "\n",
    "        return self\n",
    "\n",
    "    def __next__(self):\n",
    "\n",
    "        if self._iter is None:\n",
    "            raise StopIteration\n",
    "\n",
    "        idx, data = zip(*next(self._iter))\n",
    "        return idx, data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_image_and_labels(data: list):\n",
    "\n",
    "    imgs = torch.stack([val[0] for val in data])\n",
    "    objs = [val[1][\"objects\"] for val in data]\n",
    "    bndboxes = [obj[\"bndbox\"] for obj in objs]\n",
    "    classes = [obj[\"class\"] for obj in objs]\n",
    "\n",
    "    return dict(\n",
    "        imgs = imgs,\n",
    "        bndboxes = bndboxes,\n",
    "        classes = classes\n",
    "    )\n",
    "\n",
    "\n",
    "def train(\n",
    "    model: SSD,\n",
    "    optimiser: torch.optim.Optimizer,\n",
    "    train_data: Batcher,\n",
    "    test_data: list,\n",
    "    epochs: int,\n",
    "    test_every = 10\n",
    "):\n",
    "\n",
    "    print()\n",
    "\n",
    "    l1_losses = []\n",
    "    nll_losses = []\n",
    "\n",
    "    # setup test data\n",
    "    test_imgs = torch.stack([val[0] for val in test_data])\n",
    "    objs = [val[1][\"objects\"] for val in test_data]\n",
    "    test_bndboxes = [obj[\"bndbox\"] for obj in objs]\n",
    "    test_classes = [obj[\"class\"] for obj in objs]\n",
    "\n",
    "    test_imgs, test_bndboxes, test_classes = get_image_and_labels(\n",
    "        test_data\n",
    "    ).values()\n",
    "\n",
    "    test_ious = math_utils.batched_intersection_over_union(\n",
    "        model.default_boxes,\n",
    "        test_bndboxes\n",
    "    )\n",
    "\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "\n",
    "        for batch_idx, batch in train_data:\n",
    "\n",
    "            batch_size = len(batch)\n",
    "\n",
    "            # setup training data\n",
    "            train_imgs, train_bndboxes, train_classes = get_image_and_labels(\n",
    "                batch\n",
    "            ).values()\n",
    "\n",
    "            train_ious = math_utils.batched_intersection_over_union(\n",
    "                model.default_boxes,\n",
    "                train_bndboxes\n",
    "            )\n",
    "\n",
    "            # train\n",
    "            optimiser.zero_grad()\n",
    "\n",
    "            pred = model(train_imgs)\n",
    "\n",
    "            loss = calculate_loss(\n",
    "                model,\n",
    "                pred,\n",
    "                train_bndboxes,\n",
    "                train_classes,\n",
    "                iou = train_ious\n",
    "            )\n",
    "\n",
    "            loss[\"l1\"] /= batch_size\n",
    "            loss[\"nll\"] /= batch_size\n",
    "\n",
    "            loss[\"l1\"].backward()\n",
    "            loss[\"nll\"].backward()\n",
    "            optimiser.step()\n",
    "\n",
    "        if epoch % test_every == 0 or epoch == (epochs-1):\n",
    "        \n",
    "            with torch.no_grad():\n",
    "\n",
    "                test_pred = model(test_imgs)\n",
    "                test_loss = calculate_loss(\n",
    "                    model,\n",
    "                    test_pred,\n",
    "                    test_bndboxes,\n",
    "                    test_classes,\n",
    "                    iou = test_ious\n",
    "                )\n",
    "\n",
    "            l1_losses.append(test_loss[\"l1\"]/len(test_imgs))\n",
    "            nll_losses.append(test_loss[\"nll\"]/len(test_imgs))\n",
    "\n",
    "        print(f\"{epoch+1}/{epochs} l1 Loss: {l1_losses[-1]:.5f} nll Loss: {nll_losses[-1]:.5f}\", end=\" \"*30+\"\\r\")\n",
    "\n",
    "\n",
    "    print()\n",
    "    return l1_losses, nll_losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Proper training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dev = \"cpu\"\n",
    "model_dev = torch_utils.TORCH_DEVICE\n",
    "model = SSD(3, 5, device = model_dev, width = image_width)\n",
    "\n",
    "optim = torch.optim.Adam(model.parameters())\n",
    "\n",
    "examples = min(len(train_split), 3000)\n",
    "# not particularly the way to use Datasets, but works\n",
    "train_dat = [train_split[i] for i in range(examples)]\n",
    "test_dat = [test_split[i] for i in range(5)]\n",
    "\n",
    "batch_size = 30\n",
    "\n",
    "losses = train(\n",
    "    model,\n",
    "    optim,\n",
    "    Batcher(train_dat, batch_size = batch_size, shuffle = True),\n",
    "    test_dat,\n",
    "    epochs = 10,\n",
    "    test_every = 1\n",
    ")\n",
    "\n",
    "# TODO: check out TensorBoard\n",
    "\n",
    "fig, ax = plt.subplots(1, 2)\n",
    "\n",
    "ax[0].plot(losses[0], label=\"l1\")\n",
    "ax[0].legend()\n",
    "ax[1].plot(losses[1], label=\"nll\")\n",
    "ax[1].legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_model = SSD(\n",
    "    3,\n",
    "    5,\n",
    "    device = torch_utils.TORCH_DEVICE,\n",
    "    width = image_width\n",
    ")\n",
    "test_optim = torch.optim.Adam(test_model.parameters())\n",
    "\n",
    "\n",
    "# TODO: lots of .to calls? loss calculation seems to also be a time hog\n",
    "# Basically, the issue is having to move from GPU to CPU, and obviously\n",
    "# the lack of vectorisation in parts. Without vectorisation, calculating\n",
    "# purely on GPU seems to run slower compared to CPU.\n",
    "train_test = [train_split[i] for i in range(30)]\n",
    "test_test = [test_split[i] for i in range(1)]\n",
    "%prun -s \"cumulative\" _ = train(test_model, test_optim, Batcher(train_test, batch_size = 30), test_test, epochs=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "im, meta = validation_split[0]\n",
    "\n",
    "pred = model(im.unsqueeze(dim=0))\n",
    "\n",
    "bndbox, classes = meta[\"objects\"][\"bndbox\"], meta[\"objects\"][\"class\"]\n",
    "iou = math_utils.intersection_over_union(model.default_boxes, bndbox)\n",
    "\n",
    "cl, boxes = classes_and_boxes_truth(iou, classes)\n",
    "matched = boxes != -1\n",
    "print(boxes.unique())\n",
    "predicted = offset_default_boxes(model.default_boxes[matched], torch.hstack(pred[\"box_preds\"])[0][matched])\n",
    "\n",
    "base_im = add_bb(im, meta[\"objects\"][\"bndbox\"])\n",
    "print(torch.hstack(pred[\"box_preds\"]))\n",
    "predicted_im = add_bb(base_im, predicted, color = \"red\")\n",
    "plt.imshow(to_plottable(predicted_im))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(model.default_boxes[matched])\n",
    "print(torch.hstack(pred[\"box_preds\"])[0][matched])\n",
    "print(predicted)\n",
    "\n",
    "# compare boxes\n",
    "# TODO: Non-Maximum Suppression\n",
    "print(torch.unique(boxes))\n",
    "true_boxes = meta[\"objects\"][\"bndbox\"][boxes[matched]]\n",
    "print(true_boxes)\n",
    "print((predicted - true_boxes).abs().sum(dim=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# compare classes\n",
    "\n",
    "pred_cls = torch.hstack(pred[\"class_preds\"])[0].max(dim=1).indices\n",
    "classes_names = list(map(\n",
    "    lambda tup: tup[0],\n",
    "    sorted(validation_split.class_dict.items(), key = lambda tup: tup[1])\n",
    "    ))\n",
    "print(\"predictions per class:\",[((pred_cls == i).sum().item(),classes_names[i]) for i in range(3)])\n",
    "\n",
    "print(\"Correct predictions in total:\",(pred_cls == cl).sum().item())\n",
    "print(\"Correct predictions per class:\", [(torch.logical_and((pred_cls == i), (cl == i)).sum().item(), classes_names[i]) for i in range(3)])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate accuracies for all examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "val_imgs, val_bndboxes, val_classes = get_image_and_labels(\n",
    "    [validation_split[i] for i in range(len(validation_split))]\n",
    ").values()\n",
    "\n",
    "with torch.no_grad():\n",
    "    val_preds = model(val_imgs)\n",
    "\n",
    "val_box_preds = torch.hstack(val_preds[\"box_preds\"])\n",
    "\n",
    "val_class_preds = torch.hstack(val_preds[\"class_preds\"])\n",
    "val_class_preds = torch.max(val_class_preds, dim=-1).indices\n",
    "\n",
    "\n",
    "val_iou = math_utils.batched_intersection_over_union(model.default_boxes, val_bndboxes)\n",
    "\n",
    "class_score = {k:torch.tensor([0.0]) for k in validation_split.class_dict.values()}\n",
    "class_dict_reverse = {v:k for k,v in validation_split.class_dict.items()}\n",
    "for i in range(len(validation_split)):\n",
    "    true_val_classes, true_val_boxes = classes_and_boxes_truth(val_iou[i], val_classes[i])\n",
    "\n",
    "    for cl in map(int, true_val_classes.unique()):\n",
    "\n",
    "        is_class = true_val_classes == cl\n",
    "\n",
    "        accuracy = (\n",
    "            (val_class_preds[i][is_class] == true_val_classes[is_class]).sum()/(is_class.sum())\n",
    "        )\n",
    "\n",
    "        class_score[cl] += accuracy\n",
    "\n",
    "class_score = {class_dict_reverse[k]+\" accuracy\":(v/len(validation_split)).item() for k,v in class_score.items()}\n",
    "\n",
    "\n",
    "\n",
    "print(class_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO\n",
    "\n",
    "- Non-maximum suppression: useful for evaluating the predictions as well,\n",
    "currently kind of difficult in a sensible way at least with the box predictions\n",
    " - Better accuracy calculations (currently just the class scores)\n",
    "- Improve speed? loss calculations quite slow, GPU -> CPU -> GPU changes\n",
    "slow the training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
