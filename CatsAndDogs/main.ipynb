{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read from Kaggle, unzip file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request as urlrequest\n",
    "from pathlib import Path\n",
    "\n",
    "base_data_path = Path() / \"data\" \n",
    "\n",
    "base_data_path.mkdir(parents = True, exist_ok = True)\n",
    "\n",
    "dataset_location = base_data_path / \"dataset\"\n",
    "\n",
    "zip_path = base_data_path / \"dataset.zip\"\n",
    "\n",
    "if not zip_path.exists():\n",
    "\n",
    "    dataset_url = \"https://www.kaggle.com/api/v1/datasets/download/alvarole/asirra-cats-vs-dogs-object-detection-dataset\"\n",
    "\n",
    "    response = urlrequest.urlopen(\n",
    "        dataset_url,\n",
    "    )\n",
    "\n",
    "    download_size = response.getheader(\"Content-Length\")\n",
    "    with open(zip_path, \"wb\") as f:\n",
    "\n",
    "        f.write(response.read())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import zipfile\n",
    "import os\n",
    "\n",
    "def unzip():\n",
    "\n",
    "    inner_file = \"Asirra_ cat vs dogs\"\n",
    "    with zipfile.ZipFile(zip_path, \"r\") as zip:\n",
    "        \n",
    "        for item in zip.infolist():\n",
    "\n",
    "            zip.extract(item, base_data_path)\n",
    "\n",
    "    os.rename(base_data_path / inner_file, dataset_location)\n",
    "\n",
    "# unzip()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import itertools\n",
    "import xml.etree.ElementTree as ET\n",
    "import torch\n",
    "from typing import TypedDict\n",
    "\n",
    "\n",
    "def patched_dataset_paths(dataset_location):\n",
    "\n",
    "    return itertools.batched(dataset_location.iterdir(), 2)\n",
    "\n",
    "class Objects(TypedDict):\n",
    "    '''\n",
    "    `bndbox`: (xmin,ymin,xmax,ymax)\n",
    "    '''\n",
    "\n",
    "    name: str\n",
    "    pose: str\n",
    "    truncated: int\n",
    "    difficult: int\n",
    "    bndbox: torch.Tensor\n",
    "\n",
    "class Metadata(TypedDict):\n",
    "    '''\n",
    "    `size`: (width, height, depth)\n",
    "    '''\n",
    "\n",
    "    size: torch.Tensor\n",
    "    objects: list[Objects]\n",
    "\n",
    "class MetaWithImage(Metadata):\n",
    "\n",
    "    img_path: str\n",
    "\n",
    "# specific xml reader implementation for the lolz\n",
    "def read_metadata(xml_file: Path) -> Metadata:\n",
    "    '''\n",
    "    Read labeling from xml file into dict.\n",
    "    '''\n",
    "\n",
    "    with open(xml_file, \"r\", encoding = \"utf-8\") as f:\n",
    "        text = ET.canonicalize(from_file=f, strip_text = True)\n",
    "        \n",
    "    tree = ET.fromstring(text)\n",
    "\n",
    "    size = tree.find(\"size\")\n",
    "    size = torch.tensor([int(elem.text) for elem in size.iter() if not elem.tag == \"size\"])\n",
    "\n",
    "\n",
    "    objects = tree.findall(\"object\")\n",
    "    objects: Objects = [dict(\n",
    "        name = obj.find(\"name\").text,\n",
    "        pose = obj.find(\"pose\").text,\n",
    "        truncated = int(obj.find(\"truncated\").text),\n",
    "        difficult = int(obj.find(\"difficult\").text),\n",
    "        bndbox = torch.tensor([\n",
    "            float(elem.text) \n",
    "            for elem in obj.find(\"bndbox\").iter()\n",
    "            if not elem.tag == \"bndbox\"\n",
    "        ])\n",
    "    ) for obj in objects]\n",
    "\n",
    "    metadata: Metadata = dict(\n",
    "        size = size,\n",
    "        objects = objects\n",
    "    )\n",
    "\n",
    "    return metadata\n",
    "\n",
    "def get_dataset(dataset_location) -> list[MetaWithImage]:\n",
    "\n",
    "    meta: list[MetaWithImage] = []\n",
    "    for img, xml_path in patched_dataset_paths(dataset_location):\n",
    "\n",
    "        metadata: MetaWithImage = read_metadata(xml_path) | dict(img_path = img)\n",
    "        meta.append(metadata)\n",
    "\n",
    "    return meta\n",
    "\n",
    "def dataset_splits(dataset: list[MetaWithImage] | None = None, fractions: tuple[float] = (0.8, 0.1, 0.1)):\n",
    "\n",
    "    dataset = get_dataset(dataset_location) if dataset is None else dataset\n",
    "    return  torch.utils.data.random_split(dataset, fractions)\n",
    "\n",
    "\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision.io import read_image\n",
    "import torchvision.transforms.v2.functional as tvt\n",
    "\n",
    "class CatsAndDogsDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, data: list[MetaWithImage], resize_to = (300,300)):\n",
    "\n",
    "        self.resize_to = resize_to\n",
    "        self.data = [self.metadata_transform(val) for val in data]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def image_transform(self, img):\n",
    "\n",
    "        return tvt.resize(img, self.resize_to)\n",
    "\n",
    "    def metadata_transform(self, metadata: MetaWithImage):\n",
    "        '''\n",
    "        Transform the bndbox values to be in the range [0,1].\n",
    "        '''\n",
    "\n",
    "        resize_x, resize_y = self.resize_to\n",
    "        width, height, depth = metadata['size']\n",
    "        for i in range(len(metadata[\"objects\"])):\n",
    "            obj = metadata['objects'][i]\n",
    "            bndbox = obj['bndbox']\n",
    "            metadata[\"objects\"][i]['bndbox'] = (\n",
    "                bndbox/torch.tensor([width, height]*2)\n",
    "            )\n",
    "\n",
    "        metadata[\"size\"] = torch.tensor([resize_x, resize_y, depth])\n",
    "\n",
    "        return metadata\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        metadata = self.data[idx]\n",
    "        img_path = metadata[\"img_path\"]\n",
    "        image = read_image(img_path)\n",
    "        image = self.image_transform(image)\n",
    "        return image, metadata\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_split, validation_split, test_split = dataset_splits()\n",
    "\n",
    "\n",
    "train_split = CatsAndDogsDataset(train_split)\n",
    "validation_split = CatsAndDogsDataset(validation_split)\n",
    "test_split = CatsAndDogsDataset(test_split)\n",
    "\n",
    "print(len(train_split))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test datasets with plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import torchvision.transforms.v2.functional as vision_transforms\n",
    "from torchvision.utils import draw_bounding_boxes\n",
    "\n",
    "def to_plottable(img):\n",
    "\n",
    "    return vision_transforms.to_pil_image(img) \n",
    "\n",
    "def add_bb(img, meta: MetaWithImage):\n",
    "\n",
    "    width, height, _ = [val.item() for val in meta[\"size\"]]\n",
    "    for _object in meta[\"objects\"]:\n",
    "        bb = _object[\"bndbox\"].reshape((-1, 4))\n",
    "        bb = bb*torch.tensor([width, height, width, height])\n",
    "        print(bb)\n",
    "        img = draw_bounding_boxes(img, bb, colors = \"cyan\")\n",
    "\n",
    "    return img\n",
    "\n",
    "plt.figure()\n",
    "im, meta = train_split[0]\n",
    "im = add_bb(im, meta)\n",
    "plt.imshow(to_plottable(im))\n",
    "print(meta[\"objects\"])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test IoU calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import src.default_box as default_box\n",
    "import src.utils.math as math_utils\n",
    "\n",
    "\n",
    "im, meta = train_split[0]\n",
    "bndbox = meta[\"objects\"][0][\"bndbox\"].reshape([-1,4])\n",
    "width, height, _ = [val.item() for val in meta[\"size\"]]\n",
    "\n",
    "print(width, height)\n",
    "step = 40\n",
    "boxes = default_box.default_boxes(\n",
    "    scale = 0.7,\n",
    "    centers = default_box.default_box_centers(width, height, width_step = step, height_step = step)\n",
    ")\n",
    "\n",
    "num_boxes, num_ratios, _ = boxes.shape\n",
    "boxes = boxes.reshape([num_boxes*num_ratios, 4])\n",
    "\n",
    "print(bndbox)\n",
    "print(boxes.shape)\n",
    "iou = math_utils.intersection_over_union(boxes.reshape([num_boxes*num_ratios, 4]), bndbox)\n",
    "print(iou.shape)\n",
    "iou = iou.reshape([num_ratios, len(bndbox), num_boxes])\n",
    "print(iou.shape)\n",
    "\n",
    "print([(iou >= val).sum() for val in [0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
