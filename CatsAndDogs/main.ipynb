{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read from Kaggle, unzip file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request as urlrequest\n",
    "from pathlib import Path\n",
    "\n",
    "base_data_path = Path() / \"data\" \n",
    "\n",
    "base_data_path.mkdir(parents = True, exist_ok = True)\n",
    "\n",
    "dataset_location = base_data_path / \"dataset\"\n",
    "\n",
    "zip_path = base_data_path / \"dataset.zip\"\n",
    "\n",
    "if not zip_path.exists():\n",
    "\n",
    "    dataset_url = \"https://www.kaggle.com/api/v1/datasets/download/alvarole/asirra-cats-vs-dogs-object-detection-dataset\"\n",
    "\n",
    "    response = urlrequest.urlopen(\n",
    "        dataset_url,\n",
    "    )\n",
    "\n",
    "    download_size = response.getheader(\"Content-Length\")\n",
    "    with open(zip_path, \"wb\") as f:\n",
    "\n",
    "        f.write(response.read())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import zipfile\n",
    "import os\n",
    "\n",
    "def unzip():\n",
    "\n",
    "    inner_file = \"Asirra_ cat vs dogs\"\n",
    "    with zipfile.ZipFile(zip_path, \"r\") as zip:\n",
    "        \n",
    "        for item in zip.infolist():\n",
    "\n",
    "            zip.extract(item, base_data_path)\n",
    "\n",
    "    os.rename(base_data_path / inner_file, dataset_location)\n",
    "\n",
    "# unzip()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import itertools\n",
    "import xml.etree.ElementTree as ET\n",
    "import torch\n",
    "from typing import TypedDict\n",
    "\n",
    "\n",
    "def patched_dataset_paths(dataset_location):\n",
    "\n",
    "    return itertools.batched(dataset_location.iterdir(), 2)\n",
    "\n",
    "class Objects(TypedDict):\n",
    "    '''\n",
    "    `bndbox`: (xmin,ymin,xmax,ymax)\n",
    "    '''\n",
    "\n",
    "    name: str\n",
    "    pose: str\n",
    "    truncated: int\n",
    "    difficult: int\n",
    "    bndbox: torch.Tensor\n",
    "\n",
    "class Metadata(TypedDict):\n",
    "    '''\n",
    "    `size`: (width, height, depth)\n",
    "    '''\n",
    "\n",
    "    size: torch.Tensor\n",
    "    objects: list[Objects]\n",
    "\n",
    "class MetaWithImage(Metadata):\n",
    "\n",
    "    img_path: str\n",
    "\n",
    "# specific xml reader implementation for the lolz\n",
    "def read_metadata(xml_file: Path) -> Metadata:\n",
    "    '''\n",
    "    Read labeling from xml file into dict.\n",
    "    '''\n",
    "\n",
    "    with open(xml_file, \"r\", encoding = \"utf-8\") as f:\n",
    "        text = ET.canonicalize(from_file=f, strip_text = True)\n",
    "        \n",
    "    tree = ET.fromstring(text)\n",
    "\n",
    "    size = tree.find(\"size\")\n",
    "    size = torch.tensor([int(elem.text) for elem in size.iter() if not elem.tag == \"size\"])\n",
    "\n",
    "\n",
    "    objects = tree.findall(\"object\")\n",
    "    objects: Objects = [dict(\n",
    "        name = obj.find(\"name\").text,\n",
    "        pose = obj.find(\"pose\").text,\n",
    "        truncated = int(obj.find(\"truncated\").text),\n",
    "        difficult = int(obj.find(\"difficult\").text),\n",
    "        bndbox = torch.tensor([\n",
    "            float(elem.text) \n",
    "            for elem in obj.find(\"bndbox\").iter()\n",
    "            if not elem.tag == \"bndbox\"\n",
    "        ])\n",
    "    ) for obj in objects]\n",
    "\n",
    "    metadata: Metadata = dict(\n",
    "        size = size,\n",
    "        objects = objects\n",
    "    )\n",
    "\n",
    "    return metadata\n",
    "\n",
    "def get_dataset(dataset_location) -> list[MetaWithImage]:\n",
    "\n",
    "    meta: list[MetaWithImage] = []\n",
    "    for img, xml_path in patched_dataset_paths(dataset_location):\n",
    "\n",
    "        metadata: MetaWithImage = read_metadata(xml_path) | dict(img_path = img)\n",
    "        meta.append(metadata)\n",
    "\n",
    "    return meta\n",
    "\n",
    "def dataset_splits(dataset: list[MetaWithImage] | None = None, fractions: tuple[float] = (0.8, 0.1, 0.1)):\n",
    "\n",
    "    dataset = get_dataset(dataset_location) if dataset is None else dataset\n",
    "    return  torch.utils.data.random_split(dataset, fractions)\n",
    "\n",
    "\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision.io import read_image\n",
    "import torchvision.transforms.v2.functional as tvt\n",
    "\n",
    "class CatsAndDogsDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, data: list[MetaWithImage], resize_to = (300,300)):\n",
    "        '''\n",
    "\n",
    "\n",
    "        `resize_to`:\n",
    "        - should be square\n",
    "        '''\n",
    "\n",
    "        self.resize_to = resize_to\n",
    "        self.data = [self.metadata_transform(val) for val in data]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def image_transform(self, img):\n",
    "\n",
    "        return tvt.resize(img, self.resize_to)\n",
    "\n",
    "    def metadata_transform(self, metadata: MetaWithImage):\n",
    "        '''\n",
    "        Transform the bndbox values to be in the range [0,1].\n",
    "        '''\n",
    "\n",
    "        resize_x, resize_y = self.resize_to\n",
    "        width, height, depth = metadata['size']\n",
    "        for i in range(len(metadata[\"objects\"])):\n",
    "            obj = metadata['objects'][i]\n",
    "            bndbox = obj['bndbox']\n",
    "            metadata[\"objects\"][i]['bndbox'] = (\n",
    "                bndbox/torch.tensor([width, height]*2)\n",
    "            )\n",
    "\n",
    "        metadata[\"size\"] = torch.tensor([resize_x, resize_y, depth])\n",
    "\n",
    "        return metadata\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        metadata = self.data[idx]\n",
    "        img_path = metadata[\"img_path\"]\n",
    "        image = read_image(img_path)\n",
    "        image = self.image_transform(image)\n",
    "        return image, metadata\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_split, validation_split, test_split = dataset_splits()\n",
    "\n",
    "\n",
    "train_split = CatsAndDogsDataset(train_split)\n",
    "validation_split = CatsAndDogsDataset(validation_split)\n",
    "test_split = CatsAndDogsDataset(test_split)\n",
    "\n",
    "print(len(train_split))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test datasets with plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import torchvision.transforms.v2.functional as vision_transforms\n",
    "from torchvision.utils import draw_bounding_boxes\n",
    "\n",
    "def to_plottable(img):\n",
    "\n",
    "    return vision_transforms.to_pil_image(img) \n",
    "\n",
    "def add_bb(img, meta: MetaWithImage):\n",
    "\n",
    "    width, height, _ = [val.item() for val in meta[\"size\"]]\n",
    "    for _object in meta[\"objects\"]:\n",
    "        bb = _object[\"bndbox\"].reshape((-1, 4))\n",
    "        bb = bb*torch.tensor([width, height, width, height])\n",
    "        print(bb)\n",
    "        img = draw_bounding_boxes(img, bb, colors = \"cyan\")\n",
    "\n",
    "    return img\n",
    "\n",
    "plt.figure()\n",
    "im, meta = train_split[int(torch.rand(1).item()*len(train_split))]\n",
    "im = add_bb(im, meta)\n",
    "plt.imshow(to_plottable(im))\n",
    "print(meta[\"objects\"])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test IoU calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import src.default_box as default_box\n",
    "import src.utils.math as math_utils\n",
    "\n",
    "\n",
    "im, meta = train_split[0]\n",
    "bndbox = meta[\"objects\"][0][\"bndbox\"].reshape([-1,4])\n",
    "width, height, _ = [val.item() for val in meta[\"size\"]]\n",
    "\n",
    "print(width, height)\n",
    "boxes = default_box.default_boxes(\n",
    "    scale = 0.8,\n",
    "    centers = default_box.default_box_centers(width//8, height//8)\n",
    ")\n",
    "\n",
    "num_boxes, num_ratios, _ = boxes.shape\n",
    "boxes = boxes.reshape([num_boxes*num_ratios, 4])\n",
    "\n",
    "print(bndbox)\n",
    "print(boxes.shape)\n",
    "iou = math_utils.intersection_over_union(boxes.reshape([num_boxes*num_ratios, 4]), bndbox)\n",
    "print(iou.shape)\n",
    "iou = iou.reshape([num_ratios, len(bndbox), num_boxes])\n",
    "print(iou.shape)\n",
    "\n",
    "print([(iou >= val).sum() for val in [0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9]])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Design network and training setup\n",
    "\n",
    "See [SSD paper](https://arxiv.org/abs/1512.02325), also [Dive into deep learning](https://d2l.ai/) has good practical material and examples.\n",
    "\n",
    "SSD consists of a base network followed by successive prediction layers\n",
    "which generate the class predictions and bounding box offsets at different\n",
    "scales. \n",
    "\n",
    "- The base network downsamples the input, decreasing the width and height\n",
    "while adding more channels.\n",
    "\n",
    "- The prediction layers make the predictions\n",
    "by convolving over their input and outputting a value for each class\n",
    "and each default box offset. \n",
    "    * The output from a layer is further downsampled by e.g. pooling,\n",
    "    creating a larger receptive field for the next layer. As a result,\n",
    "    the scale of default boxes should increase further into the net.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from math import floor\n",
    "\n",
    "import src.utils.reshape as reshape\n",
    "\n",
    "def down_sampler(in_channels, out_channels):\n",
    "\n",
    "    return torch.nn.Sequential(*[\n",
    "        torch.nn.Conv2d(\n",
    "            in_channels = in_channels,\n",
    "            out_channels = out_channels,\n",
    "            kernel_size = 3,\n",
    "            padding = 1\n",
    "        ),\n",
    "        torch.nn.ReLU(),\n",
    "        torch.nn.MaxPool2d(kernel_size = 2)\n",
    "    ])\n",
    "\n",
    "def prediction_layer(in_channels, num_classes, num_ratios):\n",
    "\n",
    "    return torch.nn.ModuleDict(\n",
    "        dict(\n",
    "            class_pred = torch.nn.Conv2d(\n",
    "                in_channels = in_channels,\n",
    "                out_channels = num_classes*num_ratios,\n",
    "                kernel_size = 3,\n",
    "                padding = 1\n",
    "            ),\n",
    "            box_pred = torch.nn.Conv2d(\n",
    "                in_channels = in_channels,\n",
    "                out_channels = 4*num_ratios,\n",
    "                kernel_size = 3,\n",
    "                padding = 1\n",
    "\n",
    "            )\n",
    "        )\n",
    "    )\n",
    "\n",
    "def max_pool_change(in_size):\n",
    "    '''\n",
    "    How much an input size of `in_size` changes using\n",
    "    torch.nn.MaxPool2D(kernel_size = 2)\n",
    "    '''\n",
    "            \n",
    "    return floor((in_size - (2-1) - 1)/2 + 1)\n",
    "\n",
    "def repeat_apply(func, _input, num):\n",
    "\n",
    "    for _ in range(num):\n",
    "        _input = func(_input)\n",
    "\n",
    "    return _input\n",
    "\n",
    "def generate_default_boxes(width, scale):\n",
    "\n",
    "    return default_box.default_boxes(\n",
    "        scale = scale,\n",
    "        centers = default_box.default_box_centers(width, width)\n",
    "    )\n",
    "\n",
    "class SSD(torch.nn.Module):\n",
    "    '''\n",
    "\n",
    "    `num_classes`: int\n",
    "    \n",
    "    `num_ratios`: int\n",
    "    - Number of ratios used for the default boxes.\n",
    "\n",
    "    `default_boxes`: torch.Tensor\n",
    "    - default boxes for each feature map layer, of shape\n",
    "    (num_ratios*feature_map_height*feature_map_width,4). Indexing\n",
    "    over boxes per pixel works by [num_ratios*i:num_ratios*(i+1), 4]\n",
    "\n",
    "    `pixels_per_layer`: list of int\n",
    "    - pixels per feature map layer, useful for iterating over `default_boxes`\n",
    "    in layer-by-layer manner.\n",
    "    '''\n",
    "\n",
    "    def __init__(self, num_classes, num_ratios, in_channels = 3, width = 300):\n",
    "        '''\n",
    "        `num_classes`:\n",
    "        - Number of classes including background \"class\".\n",
    "        '''\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        base_channels = [in_channels,9,27,81]\n",
    "\n",
    "        self.num_classes = num_classes\n",
    "        self.num_ratios = num_ratios\n",
    "\n",
    "        self.max_pool = torch.nn.MaxPool2d(kernel_size = 2)\n",
    "\n",
    "        self.base_network = torch.nn.Sequential(*[\n",
    "            down_sampler(base_channels[i], base_channels[i+1])\n",
    "            for i in range(len(base_channels)-1)\n",
    "        ])\n",
    "\n",
    "        self.prediction_layers = [\n",
    "            prediction_layer(base_channels[-1], num_classes, num_ratios)\n",
    "            for _ in range(3)\n",
    "        ]\n",
    "        \n",
    "        # pre-generate the default boxes\n",
    "        # ------------------------------\n",
    "        start_size = repeat_apply(max_pool_change, width, len(base_channels)-1)\n",
    "        \n",
    "        scales = default_box.scales(len(self.prediction_layers))\n",
    "\n",
    "        def_boxes = []\n",
    "        self.pixels_per_layer = []\n",
    "\n",
    "        for i in range(len(self.prediction_layers)):\n",
    "\n",
    "\n",
    "            def_box = generate_default_boxes(start_size, scales[i])\n",
    "            pixels = def_box.shape[0]\n",
    "            self.pixels_per_layer.append(pixels)\n",
    "            # shape to match the output from the box predictions:\n",
    "            # [pixels*ratios, boxes]\n",
    "            def_box = (\n",
    "                def_box\n",
    "                .flatten(start_dim = 1)\n",
    "                .reshape([pixels*num_ratios, 4])\n",
    "            )\n",
    "            def_boxes.append(\n",
    "                def_box\n",
    "            )\n",
    "\n",
    "            start_size = max_pool_change(start_size)\n",
    "\n",
    "        self.default_boxes = torch.vstack(def_boxes)\n",
    "        # ===============================\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        '''\n",
    "        Return dict with kwords class_preds, box_preds,\n",
    "        both a list of torch.Tensor of predictions per feature map layer.\n",
    "        '''\n",
    "\n",
    "        X = self.base_network(x)\n",
    "\n",
    "        pl = self.prediction_layers\n",
    "\n",
    "        class_preds = []\n",
    "        box_preds = []\n",
    "        for i in range(len(pl)):\n",
    "\n",
    "            # predict classes\n",
    "            class_pred = reshape.items_per_pixel(\n",
    "                pl[i][\"class_pred\"](X),\n",
    "                self.num_classes\n",
    "            )\n",
    "            torch.nn.functional.log_softmax(class_pred, dim=1)\n",
    "            class_preds.append(\n",
    "                torch.nn.functional.log_softmax(class_pred, dim=1)\n",
    "                \n",
    "            )\n",
    "\n",
    "            # predict box offsets\n",
    "            box_pred = reshape.items_per_pixel(pl[i][\"box_pred\"](X), 4)\n",
    "            # restrict w/h offset to > 0 (w/h used to scale width and height)\n",
    "            box_pred[:,[2,3]] = torch.nn.functional.softplus(box_pred[:,[2,3]])\n",
    "\n",
    "            box_preds.append(\n",
    "                box_pred\n",
    "            )\n",
    "\n",
    "            X = self.max_pool(X)\n",
    "\n",
    "        return dict(\n",
    "            class_preds = class_preds,\n",
    "            box_preds = box_preds\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import accumulate\n",
    "\n",
    "\n",
    "model = SSD(3, 5)\n",
    "\n",
    "# test properties\n",
    "print(model.default_boxes.shape)\n",
    "print(model.pixels_per_layer)\n",
    "\n",
    "dummy_img = torch.rand([3,300,300])\n",
    "\n",
    "res = model(dummy_img)\n",
    "print(f\"{torch.vstack(res[\"class_preds\"]).shape=}\")\n",
    "\n",
    "bpreds = res[\"box_preds\"][0]\n",
    "print(res[\"box_preds\"][0].shape)\n",
    "\n",
    "print(model.default_boxes[:5,:])\n",
    "\n",
    "# should match in dimensions\n",
    "box_preds = torch.vstack(res[\"box_preds\"])\n",
    "def_boxes = model.default_boxes\n",
    "\n",
    "box_sum = model.default_boxes + box_preds\n",
    "print(f\"{box_sum.shape=}\")\n",
    "\n",
    "# how to go over feature layers\n",
    "layer_indices = list(accumulate(model.pixels_per_layer))\n",
    "print(layer_indices)\n",
    "\n",
    "assert torch.all(\n",
    "    torch.tensor(box_sum[layer_indices[-2]*model.num_ratios:,:].shape) \n",
    "    == torch.tensor([model.pixels_per_layer[-1]*model.num_ratios, 4])\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## utilities for calculating loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(train_split[0])\n",
    "\n",
    "\n",
    "dat = train_split[0][1]\n",
    "bndbox = []\n",
    "clslist = []\n",
    "class_dict = dict(bkg = 0, cat = 1, dog = 2)\n",
    "for ob in train_split[0][1][\"objects\"]:\n",
    "\n",
    "    bndbox.append(ob[\"bndbox\"])\n",
    "    clslist.append(class_dict[ob[\"name\"]])\n",
    "\n",
    "bndbox = torch.vstack(bndbox)\n",
    "clslist = torch.tensor(clslist)\n",
    "\n",
    "def classes_and_boxes_truth(\n",
    "    iou: torch.Tensor,\n",
    "    ground_truth_classes: torch.Tensor,\n",
    "    threshold = 0.5,\n",
    ") -> tuple[torch.Tensor, torch.Tensor]:\n",
    "    '''\n",
    "    Calculate a tensor indicating which default box is considered\n",
    "    to overlap which class and which ground truth box.\n",
    "\n",
    "    Return shape is (number of default boxes), with each element\n",
    "    a class indicator index (background is zero) or box index (-1 is\n",
    "    no box).\n",
    "\n",
    "    `iou`:\n",
    "    - The intsection-over-union of default boxes and ground truth\n",
    "    boxes, as per\n",
    "    \n",
    "    > utils.math.intersection_over_union(model.default_boxes, ground_truth_boxes)\n",
    "    '''\n",
    "\n",
    "    matches_max = iou.max(dim=0)\n",
    "    boxes = matches_max.indices\n",
    "    classes = ground_truth_classes[boxes]\n",
    "    background = matches_max.values <= threshold\n",
    "    boxes[background] = -1\n",
    "    classes[background] = 0\n",
    "    return classes, boxes\n",
    "\n",
    "iou = math_utils.intersection_over_union(\n",
    "    model.default_boxes,\n",
    "    bndbox,\n",
    ")\n",
    "\n",
    "classes, boxes = classes_and_boxes_truth(iou, clslist)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(iou.shape)\n",
    "(iou > 0.5).sum()\n",
    "\n",
    "class_preds = torch.vstack(res[\"class_preds\"])\n",
    "\n",
    "print(class_preds.shape)\n",
    "\n",
    "print(classes.shape)\n",
    "print(classes.sum())\n",
    "print((classes == 0).sum())\n",
    "print((classes == 1).sum())\n",
    "print((classes == 2).sum())\n",
    "\n",
    "torch.nn.functional.nll_loss(class_preds, classes, reduction = \"sum\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# matching predicted boxes to ground truth.\n",
    "# - Only calculate for boxes which have a matching ground truth?\n",
    "# - Only calculate for boxes which have a predicted class other than background?\n",
    "#   - Or should box predictions and class predictions be considered\n",
    "#   independent of each other? Is the box prediction layer expected\n",
    "#   to learn to match the ground truth box independent of what the\n",
    "#   target class actually is? Of course, the two are modeled as\n",
    "#   separate, so independence should be assumed, I guess.\n",
    "\n",
    "def calculate_box_loss(\n",
    "    default_boxes: torch.Tensor,\n",
    "    predicted_offsets: torch.Tensor,\n",
    "    ground_truth_boxes: torch.Tensor,\n",
    "    ground_truth_overlap_index: torch.Tensor\n",
    "):\n",
    "    '''\n",
    "    Calculate a Smooth L1 Loss between predicted object boxes\n",
    "    and actual ones. \n",
    "    \n",
    "    `default_boxes` (xmin, ymin, xmax, ymax) will\n",
    "    be offset by `predicted_offsets` (x,y,w,h): x/y is used\n",
    "    to move the entire box along an axis, allowing any real value;\n",
    "    w/h >= 0 is used to scale the box, keeping the xmin/ymin stationary\n",
    "    while increasing/decreasing the distance of xmax/ymax from the\n",
    "    former. The result of the offset is compared against the ground\n",
    "    truth box in `ground_truth_boxes` which the default box in question\n",
    "    matched against (see `classes_and_boxes_truth()`) based on \n",
    "    `ground_truth_overlap_index`.\n",
    "    '''\n",
    "\n",
    "    # calculate only for values which had a match\n",
    "    match = ground_truth_overlap_index != -1\n",
    "\n",
    "    predicted = torch.clone(default_boxes[match,:]).detach()\n",
    "    # move boxes\n",
    "    match_predicted_offsets = predicted_offsets[match]\n",
    "    predicted += match_predicted_offsets[:, [0,1,0,1]]\n",
    "    # scale boxes\n",
    "    predicted[:,[2,3]] *= match_predicted_offsets[:, [2,3]]\n",
    "\n",
    "    # match the ground truths to the default boxes based on\n",
    "    # overlap\n",
    "    indices = ground_truth_overlap_index[match]\n",
    "    actual = ground_truth_boxes[indices,:]\n",
    "\n",
    "    return torch.nn.functional.smooth_l1_loss(\n",
    "        input = predicted,\n",
    "        target = actual,\n",
    "        reduction = \"sum\"\n",
    "    )\n",
    "\n",
    "\n",
    "calculate_box_loss(\n",
    "    model.default_boxes,\n",
    "    box_preds,\n",
    "    bndbox,\n",
    "    boxes\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def calculate_losses(\n",
    "    default_boxes,\n",
    "    predicted_offsets,\n",
    "    predicted_classes,\n",
    "    ground_truth_boxes,\n",
    "    matched_classes,\n",
    "    ground_truth_overlap_index\n",
    "):\n",
    "    '''\n",
    "    Return dict of l1, nll.\n",
    "    '''\n",
    "    \n",
    "    l1 = calculate_box_loss(\n",
    "        default_boxes,\n",
    "        predicted_offsets,\n",
    "        ground_truth_boxes,\n",
    "        ground_truth_overlap_index\n",
    "    )\n",
    "\n",
    "    nll = torch.nn.functional.nll_loss(\n",
    "        predicted_classes,\n",
    "        matched_classes,\n",
    "        reduction = \"sum\"\n",
    "    )\n",
    "\n",
    "    return dict(\n",
    "        l1 = l1,\n",
    "        nll = nll\n",
    "    )\n",
    "\n",
    "calculate_losses(model.default_boxes, box_preds, class_preds, bndbox, classes, boxes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def calculate_loss(\n",
    "        model: SSD,\n",
    "        prediction,\n",
    "        ground_truth_boxes:torch.Tensor,\n",
    "        ground_truth_classes: torch.Tensor,\n",
    "        weight: float = 1.0,\n",
    "        iou = None\n",
    "):\n",
    "    '''\n",
    "    Calculate the losses for the output of `model`'s forward pass, `prediction`.\n",
    "    `ground_truth_boxes` is (N,4), containing the bounding boxes (xmin, ymin, xmax, ymax) \n",
    "    of the input image objects. `ground_truth_classes` is (N), with the elements matching\n",
    "    the true class of the bounding boxes in `ground_truth_boxes`.\n",
    "\n",
    "    `weight` is used as weight for the returned loss as l1 + weight*nll,\n",
    "    where l1 is the loss for the predicted classes, and nll the loss\n",
    "    for the predicted boxes.\n",
    "\n",
    "    `iou` can be passed if it has been calculated using\n",
    "\n",
    "    > iou = src.utils.math.intersection_over_union(model.default_boxes, target_bndbox)\n",
    "    '''\n",
    "\n",
    "    if iou is None:\n",
    "        iou = math_utils.intersection_over_union(model.default_boxes, ground_truth_boxes)\n",
    "\n",
    "    classes, boxes = classes_and_boxes_truth(iou, ground_truth_classes)\n",
    "\n",
    "    class_preds = torch.vstack(prediction[\"class_preds\"])\n",
    "    box_preds = torch.vstack(prediction[\"box_preds\"])\n",
    "\n",
    "    losses = calculate_losses(\n",
    "        model.default_boxes,\n",
    "        box_preds,\n",
    "        class_preds,\n",
    "        ground_truth_boxes,\n",
    "        classes,\n",
    "        boxes\n",
    "    )\n",
    "\n",
    "    return losses[\"l1\"] + weight*losses[\"nll\"]\n",
    "\n",
    "\n",
    "    \n",
    "calculate_loss(model, res, bndbox, clslist)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
